\chapter{Particle Filters}
\label{sec:Particle Filter}
\section{Introduction}
Particle filters, a type of Sequential Monte Carlo (SMC) methods,
are a powerful way of estimating the posterior probability distribution
a set of parameters given a timeseries of measurements. Unlike Markov 
Chain Monte Carlo (MCMC) estimation, particle filters are designed for 
time-varying random variables. The idea behind particle filters is
extremely similar to Kalman Filters; however, unlike Kalman Filters,
distributions are stored as an empircal distribution rather than the
as the first two moments of a Gaussian. Thus particle filters are 
preferred when the model is nonlinear, and thus non-gaussian.

\section{Model}
\label{sec:Particle Filter Model}
The idea of the particle filter is to build an empirical distribution
out of a large number of parameter \emph{sets}, called particles. Each
particle contains all the parameters and states needed to propagate
the model forward.  The particle filter begins with a wide distribution 
(called the Prior Distribution)
of possible particles and then, as measurements come in, weights 
particles based on the quality of their output estimates. Thus parameter sets 
that tend to give good estimations of the measurements get weighted higher
than parameter sets that give poor estimates. Although the reliance on
a prior distribution can be troublesome, when the system being modeled
has physical meaning, establishing reasonable ranges for parameters can 
actually be quite easy. Optimizing the prior can be more difficult though,
unless the system has been extensively studied.

Suppose set or stream of measurements at discrete times are given, 
$\{Y_k, k = 1, 2, 3, ... K\}$, where $K$ is infinite for a stream. 
Because $k$ is a discrete time, let $t_k$ define the continuous
time of $k$
Suppose also that there is a hidden set of state variables,
$X(t)$ that dictates the movement
of $Y(t)$, although for most of the time dealings will be
with $X_k = X(t_k)$. The goal of the particle filter is to estimate the 
\emph{distribution} of the
true parameters $\Theta$ that dictates the movement of $X(t)$.
The model also permits random motion in $X(t)$, so the 
particle filter also estimates the distribution of $X(t)$.
The only difference between the members of parameter vector
$\Theta$ and those of $X(t)$ is that the memebers of
$\Theta$ have no known update equation. Members of both vectors
are permitted to have some noise, although this
may not be explicitly stated in the model. The generic, continuous, nonlinear
system definition is shown in \autoref{eq:GenericNonlinear}.

\begin{eqnarray}
\dot{X}(t) = f(t, X(t), u(t), \theta, \nu_x) \nonumber \\
Y(t) = g(t, X(t), u(t), \theta, \nu_y)
\label{eq:GenericNonlinear}
\end{eqnarray}

$X(t)$ is vector of state variables, $\Theta$ is a vector of system
constants, $u(t)$ is an input, $Y(t)$ the observation, and
$\nu_x$ and $\nu_y$ are random variates. Although any of these
variables could be a vector, for the sake of simplicity only
$\Theta$ and $X(t)$ will be considered as such. 

Although not necessary for particle filters in general, a few  simplifying
assumptions are made for this work. First, the systems are assumed to be 
time invariant. This 
assumption is based on the idea that if you paused the system for $\Delta t$
seconds, when unfrozen the system would continue as if nothing happend. 
Few biological systems are predictable enough for them to be summarized
by a time varying function, least of all the brain. While heart beats are certainly
periodic and have an effect on the BOLD signal, the period varies too much
for the system to be considered varying with time. 
Next, its assumes that input cannot directly
influence the output, which in the case of the BOLD signal is a good assumption.
Also, noise is considered to be additive.
Finally, because the only difference between the members of $X(t)$ and 
$\Theta$ is an update function, from now on $x$ will contain 
$\Theta$. The assumptions now allow for a simplified version of the
state space equations:

\begin{eqnarray}
\dot{X}_k = f(X_{k-1}, u_k) + \nu_x 
\label{eq:stateass}\\
Y_k = g(X_k) + \nu_y 
\label{eq:measass}
\end{eqnarray}

\section{Derivation}
The goal of the particle filter is to evolve an empirical distribution 
$P(x_k | u_{0:k}, Y_{0:k})$,
that asymptotically approaches the true probability distribution $P(X_k | u_{0:k})$.
Note that capital $X$ will be used as the actual realizations of 
the state variable, whereas $x$ will denote estimates of $X$.
Additionally, the notation $a:b$ indicates the set $[a,b]$,
as in $u_{a:b}$, which would indicate all the inputs from time $a$ to time $b$.
Considering the noise present in $X$,
 $P(X_k | u_{0:k})$ is not a single true value but probability distribution. 

To begin with, the particle filter must be given a prior distribution, from
which the initial $N_p$ particles are drawn. A particle contains a weight
as well as an estimate of $X_k$, which as already stated, contains every
variable needed to run the model. Then the prior is generated from a 
given distribution, $\alpha(X)$, by:

\begin{equation}
\{[x^i_0,w^i] : x^i_0 \sim \alpha(X), w^i = \frac{1}{N_p}, i \in \{1, 2, ... , N_p\} \}
\end{equation}

Where $N_p$ is the number of particles or points used to describe the prior 
using a Mixture PDF. 
Note that any exponents will be explicitly labeled as such, to avoid confusion with
the particle numbering scheme. 

Therefore, after the particle have been generated they should approximate $\alpha(X)$:

\begin{equation}
\alpha(X) \approx P(x_0) = \sum_{i=0}^{N_p} w^i\delta(X - x^i_0 ) dx
\end{equation}
Where $\delta(x-x_0)$ is 1 if and only if $x = x_0$ (the Kronecker delta function).

If a flat prior is 
preferred, then each particle's weight could be scaled to the reciprocal of the
density at the particle: 
\begin{equation}
w^i = \frac{1}{\alpha(x^i_0)}
\end{equation}
Whether or not to flatten the prior is a design decision. The reason this might 
be preferred over a direct
uniform distribution is that the distribution width will inherently scale 
for increased particle counts although some distributions
flatten out better than others. Either way, $\alpha(X)$ \emph{must} be
wide enough to incorporate any posterior that arises. If the prior is
not sufficiently dense, the particle filter can compensate, if it is
not sufficiently wide the particle filter won't converge. 

%todo, put back?
%Once the probability, $P(x_k | x_{0:k-1}, y_{0:k-1})$ has been found
%(initially this is just Mixture approximating the prior since no measurements are 
%available and no previous probabilities are available), its possible to approximate
%the probability between times when measurement is available, by shifting
%the probability according the progression of the state equations. This is only 
%an approximate, since integrating $\nu_d$ should increase uncertainty as
%time without a measurement passes. 
%
%\begin{equation}
%P(x(T+\Delta t)) \approx 
%\sum_{i=1}^{N_p} w_i\delta\left(x - (x_i(T) + \int_T^{T+\Delta} \dot{x}_i(t) dt) \right)
%\end{equation}

\subsection{Weighting}
For all the following areas, the probabilities implicitly depend on $u_{0:k}$, 
so those terms are left off for simplicity.

Whenever a measurement becomes available it permits refinement of the
posterior density.
This process of incorporating new data is called sequential importance sampling,
and eventually allows convergence. The weight is defined as
\begin{equation}
w^i_k \propto \frac{P(x^i_{0:k} | y_{0:k})}{q(x^i_{0:k} | y_{0:k})}
\label{eq:weightfunc}
\end{equation}
where $q$ is called an \emph{importance density}. The importance density
is the density of the points, thus by dividing by this value, the weight
should not depend on the location of the estimation points, but rather
only on $P(x^i_{0:k} | y_{0:k})$, the probability of that particle
being correct given all the measurements up to time $k$. 
Of course if there is a far off peak in
the posterior that $q$ does not have support points in, there will 
be quantization errors, and that part of the density can't be modeled. This is why
it is absolutely necessary that $q$ fully covers $P(x^i_{0:k} | y_{0:k})$.

It is helpful
to consider how the importance density affects the initial distribution. 
In the initial distribution, the weights are all the same; and for
the sake of argument, let them all be scaled up to 1. Then
\begin{equation}
w^i_k q(x^i_{0:k} | y_{0:k}) = q(x^i_{0:k} | y_{0:k}) = P(x^i_{0:k} | y_{0:k})
\end{equation}
the estimated probability, $P(x^i_{0:k} | y_{0:k})$ depends only on the 
way the particles are distributed. As new measurements are incorporated,
the weight will accumulate probabilities through time, which will be discussed
next. 

\section{Calculating Weights}
To calculate the weight of a particular particle, it is necessary to 
calculate both $q(x^i_{0:k} | y_{0:k})$ and $P(x^i_{0:k} | y_{0:k})$.
Note that $q(x^i_{0:k} | y_{0:k})$ may be simplified by assuming that 
$y_k$ doesn't contain any information about $x_{k-1}$. Technically this 
could be false; since later measurements may shed light on currently hidden
changes in $x$. For practical applications though it is helpful assumption.
\begin{equation}
q(x^i_{0:k} | y_{0:k}) = q(x^i_{0:k} | y_{0:k-1})
\label{eq:QAssump}
\end{equation}
The choice of the importance density is another design decision; however
it is common to use the integrated state equations. 
Although other importance density functions exist; for the particle filter
used here, the standard importance density will be used: the modeled
prior.
\begin{equation}
q(x_k | x_{k-1}, y_{0:k}) =  P(x_k | x_{k-1})
\label{eq:ImportanceDensity}
\end{equation}
The benefit of this choice for importance density is that an approximation for
$P(x_k | x_{k-1})$ is freely available: its simply the set of particles propagated
forward in time using the state equations. Additionally it makes
updating weights extremely simple, as seen in \autoref{eq:weightevolve}.

The $q(x^i_{0:k} | y_{0:k})$ may then be simplified:
\begin{equation}
\begin{array}{cclr}
q(x_{0:k} | y_{0:k}) & = & q(x_k | x_{0:k-1}, y_{0:k})q(x_{0:k-1} | y_{0:k}) &  \\
& = & q(x_k | x_{0:k-1}, y_{0:k})q(x_{0:k-1} | y_{0:k-1})  & \text{[\autoref{eq:QAssump}]} \\
& = & q(x_k | x_{k-1}, y_{0:k})q(x_{0:k-1} | y_{0:k-1})  & \text{[Markov Property]}\\
& = & P(x_k | x_{k-1})q(x_{0:k-1} | y_{0:k-1})  & \text{[\autoref{eq:ImportanceDensity}]}
\end{array}
\end{equation}

Calculating $P(x_{0:k} | y_{0:k})$ is a bit more involved. 
First, using the assumption that the distribution of $y_k$ is 
fully constrained by $x_k$, and that $x_k$ is similarly fully 
constrained by $x_{k-1}$, we are able to make the very good assumptions that:
\begin{eqnarray}
P(y_k | x_{0:k}, y_{0:k-1}) &=& P(y_k | x_k) \nonumber \\
P(x_k | x_{0:k}, y_{0:k-1}) &=& P(x_k | x_{k-1})
\label{eq:MarkovProperty}
\end{eqnarray}
These are of course just re-statements of the state equations assumed by \autoref{eq:stateass}
and \autoref{eq:measass}.

Additionally, for the particle filter $y_k$ and $y_{0:k-1}$ are 
constant across all particles, thus $P(y_k| y_{0:k-1})$ can
be dropped when the equality is changed to a proportion. 
Using these properties, $P(x^i_{0:k} | y_{0:k})$ may be broken up as follows 
(mostly using Bayes' Theorem):
\begin{equation}
\begin{array}{lclr}
P(x_{0:k} | y_{0:k}) & = & \frac{P(y_{0:k}, x_{0:k})}{P(y_{0:k})} & \\
 & = & \frac{P(y_k, x_{0:k} | y_{0:k-1}) \cancel{P(y_{0:k-1})}}{P(y_k | y_{0:k-1}) \cancel{P(y_{0:k-1})}} & \\
 & = & \frac{P(y_k| x_{0:k}, y_{0:k-1}) P(x_{0:k} | y_{0:k-1})}{P(y_k | y_{0:k-1}) } & \\
 & = & \frac{P(y_k| x_{0:k}, y_{0:k-1}) P(x_k | x_{0:k-1}, y_{0:k-1}) P(x_{0:k-1} | y_{0:k-1})}{P(y_k | y_{0:k-1})} &  \\
& = & \frac{P(y_k| x_k) P(x_k | x_{k-1}) P(x_{0:k-1} | y_{0:k-1})}{P(y_k | y_{0:k-1})}  & [\text{\autoref{eq:MarkovProperty}}]\\
& \propto & P(y_k| x_k) P(x_k | x_{k-1}) P(x_{0:k-1} | y_{0:k-1}) & [P(y_k|y_{0:k-1}) \text{ is constant}]
 \end{array}
 \label{eq:UpdateBayes}
\end{equation}
Plugging \autoref{eq:ImportanceDensity} and the result of \autoref{eq:UpdateBayes}
into \autoref{eq:weightfunc} leads to:
\begin{eqnarray}
w^i_k & \propto & \frac{P(y_k| x^i_k) \cancel{P(x^i_k | x^i_{k-1})} P(x^i_{0:k-1} | y_{0:k-1})}
                         {\cancel{P(x^i_k | x^i_{k-1})}q(x^i_{0:k-1} | y_{0:k-1})} \nonumber \\
& \propto & w^i_{k-1}P(y_k| x_k) 
\label{eq:weightevolve}
\end{eqnarray}

Thus, by making the following relatively easy assumptions, evolving a posterior
density  requires no knowledge of noise distribution.
\begin{enumerate}
\item $f(t, x(t), u(t)) = f(x(t), u(t))$ and $g(t, x(t), u(t)) = g(x(t))$ 
\item The PDF $q(x_i(0))$ (the prior) fully covers $P(x_i(0))$
\item Markov Property: $P(x_k | x_{0:k-1}) = Pr(x_k | x_{k-1})$
\item $q(x_{0:k-1} | y_{0:k}) = q(x_{0:k-1} | y_{0:k-1})$
\end{enumerate}

\subsection{Basic Particle Filter Algorithm}
From the definition of $w_i$, the algorithm to calculate
an approximation of $P(X(t_k) | Y_{0:k})$ or $P(X(t_k + \delta t) | Y_{0:k})$
is relatively simple.

\begin{algorithm}
\caption{Sequential Importance Sampling}
\begin{algorithmic}
\STATE Initialize Particles:
\FOR{$i$ : each of $N_p$ particles }
    \STATE $x^i_0  \sim \alpha(X)$
    \STATE $w^i_0 = \frac{1}{N_p}$
\ENDFOR
\FOR{$k$ : each measurement}
    \FOR{$i$ : each particle }
        \STATE $x^i_k = x^i_{k-1} + \int_{t-1}^t f(x(\tau), u(\tau)) d\tau $
        \STATE $w^i_k = w^i_{k-1}P(y_k | x_k)$
    \ENDFOR
\ENDFOR
\STATE $P(x(t_k+\Delta t)) \approx 
\sum_{i=0}^{N_p} w^i_k \delta\left(x - (x^i_k + \int_{t_k}^{t_k+\Delta t} f(x(\tau), u(\tau)) d\tau) \right)$
\end{algorithmic}
\end{algorithm}

\subsection{Resampling}
\label{sec:Particle Filter Resampling}
As a consequence 
of the wide prior distribution (required for a proper discretization of a continuous
distribution), there will be many particles with insignificant weights. While this does help
describe the tails of the distribution, it means a lot of computation will be based.
Instead, it would be preferable if most of the computation is spent on the most probable regions.
Ideally the computation time spent on tails would be proportional to the actual size of the
tails. In this case particle locations would match the true posterior and all weights would
be equal.  The case where a large number of the weights have become irrelevantly small
is called "particle degeneracy". In  \cite{Liu1998b}
an ideal calculation of the "effective" number of particles is found based on the 
particles' "true weight". However, given that only an approximation for the true weight 
exists, they also provide a simple heuristic calculation of $N_{eff}$.
\begin{equation}
N_{eff} \approx \frac{\sum_{i=0}^{N_p} w_i}{\sum_{i=0}^{N_p} w_i^2}
\label{eq:neff}
\end{equation}
Any quick run of a particle filter will reveal that unless the prior is particularly accurate,
$N_{eff}$ drops precipitously.  To alleviate this problem
a common technique known as resampling may be applied. The idea of resampling is to 
draw from the approximate posterior, thus generating a replica of the posterior with 
a better support. Therefore, a new set of particles may be drawn from the empirical
distribution as follows:
\begin{equation}
\hat{x}_j \sim \left(\sum_{i=0}^{N_p} w^i_k\delta(x - x^i_k)\right)
\end{equation}

\begin{algorithm}
\caption{Stratified Resampling Algorithm}
\begin{algorithmic}

\end{algorithmic}
\end{algorithm}

Unfortunately, this isn't necessarily the truth: since the support is
still limited to the original particles, the number of unique particles can only go down.
This effect, often dubbed "particle impoverishment" can result in excessive quantization
errors in the final distribution. However, there is a solution. Instead of sampling from the
discrete distribution, a smoothing kernel is applied, and $\hat{\chi}_j$ are drawn from
that distribution. Because the distribution is continuous, there is no way for a collapse
of the particles to occur. The question then, is how to decide on the smoothing kernel. 
Often times the easiest way to sample from the continous distribution is to break the 
re-sampling down into two steps. First a member of the discrete distribution is randomly
selected based on the weights, and then based on the smoothing a nearby state variable 
is selected. The process of the selection will be defined as:
\begin{equation}
\chi_i = x_i + h\sigma \epsilon
\end{equation}
Where $h$ is the bandwidth, $\sigma$ is the standard deviation such that $\sigma \sigma^T = cov(x)$
and $\epsilon$ is drawn from the chosen kernel.
It has been proven that when all the elements of the mixture
have the same weight, as is the case after basic resampling, the kernel that minimizes the 
MSE between the estimated and true posterior is the Epanechnikov Kernel (cite Improving Regularised
Particle Filters, C Musso, N Oudjane and F LeGrand). 
\begin{equation}
K = \left\{
\begin{array}{lr}
\frac{n_x+2}{2c_{n_x}}(1-\|x\|^2) & if\ \|x\| < 1\\
0 & otherwise
\end{array}\right.
\end{equation}
%<more here>

\subsection{Weighting Function}
Because $P(y_k | x(T))$, what I will call the weighting function,
is based on an unknown distribution, it is necessary to decide on a function
that will approximate $P(y_k | x(T))$. Obviously the function, $\omega(y_k, f(x(t))$
needs to be centered at zero and have a scale comparable to the signal levels.
Obviously if the actual noise present in $y_k$ were to be known, then that would
be the best distribution for $\Omega$. In that case, particles that fell
far out on that distribution would be statistically impossible representations
of the system, and it would be completely reasonable to throw such particles away.
While a Gaussian function is the natural choice, because this distribution
and weight are unknown we wanted to try distributions
with wider tails, so that outliers don't completely destroy particle's weights
(and thus convergence proceeds more slowly).

Another natural choice might be one of the robust estimator weight functions, for
example the Huber or bi-square. For the purpose of this work we will stick
with long tailed distributions, however it is worth noting that long tails
may not be the optimal choice in all, or even this situation. The justification
for long tailed distributions is that we believe the noise to be long tailed,
and the variance of the noise is not well known.

Therefore, we tried three weighting functions based on three distributions: Gaussian, 
Laplace and the Cauchy. The standard deviation of the distribution is extremely
important to the convergence of particle filter. A standard deviation that is 
too large will not allow the distribution to converge in any reasonable number of 
measurements. A standard deviation below the standard deviation of the noise 
will cause the algorithm to throw out perfectly acceptable particles. 
The weighting function ultimately will shape the output distribution, $P[y]$, into that
distribution, however the distribution of $x$ will still approach a reasonable
estimate of its true distribution. Even an overly wide weighting function, will
allow the Gaussian Mixture estimate of $X$ to converge to the correct location 
parameters of the "real" posterior distribution, though the scale parameters may 
be overly large.

A reasonable method of setting the standard deviation of $\Omega$ may be by 
taking a small sample from "resting" data and using the sample standard deviation.
Since this is the first attempt at using particle filters for modeling the 
BOLD model, in this work we set the standard deviation manually at <weight standard dev>,
because it gives a more consistency and control. Of course this could be taken
further, by testing the sample data against a set of stock distributions
and choosing the best fit. This of course depends on having enough samples
to make a reasonable inference, which may not always exist. 

\section{Simple, Nonlinear Example}
A typical half wave rectifier takes a AC voltage circuit and removes
one half (say the negative half) of the signal. The resulting waveform
is still not DC, however it is then possible to use a capacitor to 
smooth the signal into something similar to DC, as shown in \autoref{fig:HalfWaveIO}.
There are other, more
complex circuits that convert the negative portion into positive and
waste less energy but but here we will keep the system simple.
Thus, let us consider a simple half wave rectifier circuit, shown in 
\autoref{fig:HalfWaveRectifier}.

The half wave rectifier circuit smoothes the gaps between high voltage
with a capacitor. Thus, when $u(t)G$ is less than $v_t$, the circuit will 
discharge the capacitor and maintain a non-zero voltage,
but when $u(t)G$ is greater than $v_t$, the output voltage will be set
by $u(t)G$ and the capacitor will charge up. We will assume a very simple
model for all the components, ignoring the complex nonlinear behavior
that can occur in a diode. 

\begin{figure}
\centering
\begin{circuitikz}[scale=2, american]
\draw
 (0,0)  node[transformer core] (T) {}
 (T.A1) -- (-1,0)
 (T.A2) -- (-1,-1.05)  to[V, v=$u(t)$] (-1, 0)
 (T.B1) -- (.5, 0) to[D, l=$v_t$] (1.5,0) to[C=$C$] (1.5, -1.05)
 (1.5, 0) -- (2.5, 0) to[R=$Rm$, v=$v_y$] (2.5, -1.05) -- (T.B2) 
 (T.base) node {G}
 (T.B1) to[open, *-*, v=$V_1$] (T.B2); 
\end{circuitikz}
\caption{An Example Half Wave Rectifier Circuit, where $G$ is the transformer
gain, $v_t$ is the activation voltage of the diode, $u(t)$ is the input at time $t$, 
$C$ is the capacitance, $R$ is the load resistance and $v_y$ is the output voltage}
\label{fig:HalfWaveRectifier}
\end{figure}

\begin{figure}
\centering
\caption{Example Input/Output of the Half Wave Rectifier}
\label{fig:HalfWaveIO}
\end{figure}

Although rectifiers are typically thought
of as receiving an AC circuit 60 Hz, we will ignore such specifics and 
assume the voltage across the output of the transformer is simply a scalar
multiple of the input voltage.  As discussed in the \autoref{sec:Particle Filter Model}
any variable with uncertainty must be part of the state variable. Therefore
the state variable will be: $X(t) = \{G, v_t, C, R_m, v_y\}$. Of course,
$u(t)$ cannot be allowed to be a square wave in such a system, since that
such a signal would never get across the transformer and regardless it
would necessitate an unrealistic infinite current across the capacitor.
The state equations would then be 

\begin{equation}
v_y(t)  = f(v_y(t-1, u(t)) =  \begin{cases} 
        u(t)G & \text{ if }  u(t)G-v_y \ge v_t\\
        v_y(t-1)\left(1 - \frac{\delta t}{R_mC}\right) & \text{ if }  u(t)G-v_y < v_t
    \end{cases} 
\end{equation}

To run the particle filter is relatively easy then, since there exists 
a recursive definition of the dynamic state variable, $v_y$. To start
with, an initial distribution must be assumed and while at first a 
Gaussian seems like a good idea, all the static state variables are strictly
positive and thus not well suited to the Gaussian. In this case then,
it would be wise to start with a Gamma distribution, and just be wary of
any standard deviation that gets larger than the prior mean. We will define
the gamma distribution as follows:

\begin{equation}
X \sim Gamma(k, \theta) \rightarrow f(x) = x^{k-1}\frac{e^{-x/\theta}}{\theta^k\Gamma(k)}
\end{equation}

where $\Gamma$ is the gamma function.
The in some ways the margin for error is decided by the weighting function, which
here will define as $W(V_y, v_{yi})$, where $V_y$ is the actual measurement, $v_y$ is the 
estimate based on all the particles, and 
$v_{yi}$ is the estimate by a particular (i$^{\text{th}}$) particle. The choice of this function is difficult,
and although the Gaussian is typically used, we found the exponential helpful
in dealing with particle deprivation.  The algorithm will then look like the following,

\begin{algorithmic}
\STATE Initialize $N_p$ Particles:
\FOR{$i$ in $N_p$}
    \STATE $G \sim Gamma(\frac{\mu^2_G}{\sigma^2_G}, \frac{\sigma^2_G}{\mu_G})$
    \STATE $v_t \sim Gamma(\frac{\mu^2_{v_t}}{\sigma^2_{v_t}}, \frac{\sigma^2_{v_t}}{\mu_{v_t}})$
    \STATE $C \sim Gamma(\frac{\mu^2_C}{\sigma^2_C}, \frac{\sigma^2_C}{\mu_C})$
    \STATE $R_m \sim Gamma(\frac{\mu^2_R}{\sigma^2_R}, \frac{\sigma^2_R}{\mu_R})$
    \STATE $v_y = 0$, (Assume the system has been off for a long time)
    \STATE let $X_i(0) = \{G, v_t, C, R_m, v_y\}$
    \STATE let $w_i(0) = 1$ or to make a flat prior, $w_i(0) = \frac{1}{Pr(X_i(0))}$ 
\ENDFOR
\STATE Run the Filter:
\FOR{$t$ in Set of Measurement Times}
    \FOR{$i$ in $N_p$}
        \STATE $v_{yi}(t) = f(v_{yi}(t-1), u(t))$
        \STATE (All other members of $X_i(t)$ remain the same)
        \STATE $w_i(t) = w_i(t-1)W(V_y(t), v_y(t))$
    \ENDFOR
\ENDFOR
\end{algorithmic}

Initially the particles will have the same output, $0$, however, as $u(t)$
changes, the response of each particle to that input will result in different
outputs. Particles that have a $v_{yi}$ near $V_y$ will be weighted higher,
and others farther away will be weighted lower. As the particle filter
runs, weights will compound resulting in a distribution that asymptotically
approaches the true joint distribution of the $X(t)$.  Of course, as we
mentioned in \autoref{sec:Particle Filter Resampling}, particles weighted zero do not significantly
contribute to the empirical distribution, so re-sampling may be necessary.
If the noise is assumed to be Gaussian then it is possible to further optimize. 
Thus we let $h$ be defines as:
\begin{eqnarray}
h = [N_s8c^{-1}_{n_x}(n_x + 4)(2\sqrt{\pi})^{n_x}]^{\frac{1}{n_x +4}}
\end{eqnarray}
and although it is very possible the underlying noise is non-gaussian, the Gaussian
may work, but sub-optimally. It has been proposed that (Monte Carlo Approximations for
General State-Space Models, markus Hurzeler and Hans R. Kunsch) if the underlying 
distribution is non-Gaussian, then using this bandwidth will oversmooth. 
In reality over smoothing
should not be too great an issue because the smoothing is only being applied to find new
particles. If the distribution is over smoothed then the algorithm may not converge as rapidly;
however, because the bandwidth is still based on particle variance, which will decay as 
particles are ruled out, it is still able to converge. In fact over smoothing is preferrable
to under smoothing, since the latter would result in false negatives, but the previous only
results in a slower decay of the variance. 
At the same time, as $n_x$, the number of dimensions in
$x$, goes to infinity, the standard deviation based approximation becomes less effective
(cite a Tutorial on Particle Filters for on-line non-linear non-gaussian bayesian
tracking, sanjeev arulampalam, simon maskell, neil gordon...).  Because of the high dimensionality of our system,
and limited measurements, it is helpful to have a broader bandwidth to explore the distribution. 
Nevertheless, because 
of the potentially wide smoothing factor applied by regularized resampling, performing this
step at every measurement would allow particles a great deal of mobility. This mobility is
the enemy of convergence, which is why regularized resampling should only be done when
$\hat{N}_{eff}$ drops very low (say less than 50). Other than the periodic regularized
resampling then, the regularized particle filter is nearly identical to the basic sampling
importance sampling filter (SIS). 

 \begin{algorithmic}
\STATE Initialize $N_p$ Particles: 
        $\{x_i(0),w_i(0) : x_i(0) \sim \alpha(x), w_i(0) = \frac{1}{N_p}, i \in \{1, 2, ... , N_p\} \}$
\STATE $T$ = \{Set of Measurement Times\}
\FOR{$t$ in $T$}
    \FOR{$i$ in $N_p$}
        \STATE $x_i(t) = x_i(t-1) + \int_{t-1}^t f(x(\tau), u(\tau)) d\tau $
        \STATE $w_i(t) = w_i(t-1)P(y(t) | x(t))$
    \ENDFOR

    \STATE Calculate $N_{eff}$ with \autoref{eq:neff}
    \IF{$N_{eff} < N_R$ (recommend $N_R = min(50, .1N_p)$ )}
        \STATE Calculate empirical $\sigma$ 
        \STATE $h = [N_s8c^{-1}_{n_x}(n_x + 4)(2\sqrt{\pi})^{n_x}]^{\frac{1}{n_x +4}}$
        \STATE Redraw particles using (stratified) basic resampling
        \FOR{$i$ in $N_p$}
            \STATE Draw $\epsilon \sim K$
            \STATE $x_i = x_i + h \sigma \epsilon$
        \ENDFOR
    \ENDIF
\ENDFOR

\STATE At $t + \Delta t$, $t \in T$, $P(x(t+\Delta t)) \approx 
\sum_{i=1}^{N_p} w_i(t)\delta\left(x - (x_i(t) + \int_t^{t+\Delta t} f(x(\tau), u(\tau)) d\tau) \right)$

 \end{algorithmic}

The ultimate effect of this regularized resampling is a convergence similar to simulated annealing
or a genetic algorithm. Versions of $x$ that are "fit" (give good measurements) spawn more children 
nearby which allow for more accurate estimation near points of high likelihood. 
As the variance of the estimated
$x$'s decrease, the radius in which children are spawned also decreases. Eventually the radius
will approach the width of the underlying uncertainty, $\nu_x$ and $\nu_y$.

