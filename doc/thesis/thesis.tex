\documentclass{article}
\usepackage[left=1cm,top=2cm,right=1cm,nohead,nofoot]{geometry}

%\usepackage{times}
%\usepackage{tikz}
%\usetikzlibrary{trees}
%\usetikzlibrary{shapes,snakes}
\usepackage{verbatim}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{setspace}
\usepackage{graphicx}

%\doublespacing

\newcommand{\R}{\mathbb{R}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\qed}{\\$\Box$}
\newcommand{\qle}{\stackrel{?}{\le}}
\newcommand{\qeq}{\stackrel{?}{=}}
\newcommand{\closure}{\overline}
\newcommand{\intersect}{\cap}
\newcommand{\union}{\cup}
\newcommand{\nullset}{\emptyset}
\newcommand{\minus}{\ \backslash\ }


\begin{comment}

:Author: Micah Chambers
\end{comment}

\begin{document}
\section*{Introduction}
Functional Magnetic Resonance Imaging (FMRI) is a powerful tool in the analysis
neural activity. Despite its rather limited temporal resolution, FMRI still
is the best way of measuring neural activity deep in the brain. 
Whereas other methods
of analyzing neural signals can be invasive or difficult to acquire, 
FMRI is relatively quick and cheap, and its analysis is straight forward.
Because of these benefits, FMRI continues to have great potential for
studying human cognition. Despite the fact that FMRI is so widely used,
the choice of analysis tools is still relatively limited. Every widely
available analysis tool is based on parametric modeling, which \emph{requires}
prior knowledge of the noise, and depends on that knowledge being accurate.
This limitation casts a long shadow over any experiment that claims to be
able to reject the null hypothesis, no matter the P-value. While obviously
there is a correlation between the BOLD signal and actual activity, its
possible the current parametric tests are over-estimating the actual amount
of activity. Its worht noting that the opposite could also be true, parametric
based t-tests may in fact underestimate activity. 

fact, typical activation detection does little to account for variations
in the shape of the signal, but instead is a direct T-test between the measured signal
and the expected shape of the signal. Because of these limitations,
multiple comparison tests are imperative (ref fish), and active regions
must be very large to be detected with any confidence. This paper
will introduce a more robust method of analyzing FMRI signals for 
activation, by means of estimating the BOLD parameters.

\subsection*{FMRI?}

\subsection*{BOLD Physiology}
The physiology that leads to the BOLD model has been well studied over the 
past decade and a half. While some uncertainty remains, for instance in
relation to the BOLD post-stimulus undershoot <papers relating to back and
forth on post-stimulus undershoot>, generally the model fits reality very 
well. As with every piece of tissue in the body, the brain requires oxygen
to extract energy from glucose in the blood. This process of removing oxygen
from red blood cells sets into motion a chain of events

inherently alters the local ratio of Oxygenated Hemoglobin
to De-oxygenated hemoglobin, which of course changes the atomic makeup of the
region. Of course MRI's contrast is based on the nuclear spin excitability of
atoms, and so change in the amount of oxygen changes the local oxygen concentration of 
The changes in the FMRI signal result from the changes in the ratio
of Oxygenated Hemaglobin to Deoxygenated Hemaglobin.

\section*{Current Techniques}
\subsection*{Basic T-Test}
\subsection*{General Linear Model}

\section*{Proposed Approach}
\subsection{Modeling}
The short term goal of current neurological models is to
learn something about the system being modeled.
While control (for instance in prosthetics), or predication
(such as BlueBrain) may some day be feasible, in the
short run we still have a great deal to learn about how our
brain works. By learning about specific systems (the dynamics
of a single person's brain), we may be able to diagnose diseases,
find their causes and even someday rectify the cause. Of course
by learning about various specific systems we can also learn 
about the general way our brains work. The reason then, for 
studying the BOLD model then is
to be able to derive the various system parameters and understand
why one brain region (say a voxel) reacts in one way, when another
acts completely differently.

While the General Linear Model has been extremely effective at
gleaning information about which voxels are "active" and which
are not, its purpose is only to say which regions are "statistically
significant". However, considering that voxels consist of millions
of neurons, the reality is that a
region could be active, but at some other level or time constant
than expected; and be completely missed by the linear model. 
Studies showing activation
in Cadavers of Phantoms highlight the inherant problem with the 
General Linear Model: a problem that mandates multiple
comparison testing with very high thresholds to ensure any 
sort of face validity \citep{drift}.
By moving to models that actually "learn" the 
parameters of the model, model error will be drastically
reduced allowing for much more reasonable statistical thresholds.
\subsection{Particle Filters}

\section*{Methods}
\subsection*{Preprocessing}
After FMRI data has been acquired it is always necessary to modify the
data in some way to make different runs comparable. Because FMRI signal
levels are essentially unit-less, at the very least it is necessary to convert
the data into \% difference from the baseline. Finding the baseline is the
first hurdle in dealing with FMRI. Various methods exist for doing this,
but they are all ad-hoc. It is common to treat each stimulus pulse independently,
ignoring the long decay of the BOLD signal. By treating each input separately,
the "base" for calculating the percent difference is merely the signal level 
before the stimulus is applied. The problem with this is that the BOLD signal
can be heavily delayed, and fall times can be as long as 10 or 15 seconds,
meaning that the assumption is clearly a false one. Another popular method 
of dealing with this problem, is to put all the time series through a high
pass filter. This will of course remove the DC component of the signal, and 
some amount of so called "drift". The problem with this method is that it is
not adaptive to the input. Huge variations in drift frequencies can exist 
in a single time-series. Thus, a single cutoff frequency could miss a significant
drift component, or it could remove \emph{actual} signal, if the cutoff frequency is
set too high. 


\section*{Introduction to Modeling}

Of course, it is well known that many of the assumptions 
that the GLM makes about BOLD activation do not hold. 
The General Linear Model ignores
the fact that the BOLD response is nonlinear, contains non-gaussian
noise and most importantly that brain networks are 
so called "Small World Networks" \citep{smallworld}, \citep{noise}. 
Merely accounting for
nonlinearities in the time-series of the BOLD response can
result in significantly better estimation as shown in \citep{nonlinearmodels}.
But even more crucial is the recent movements away from mass-
univariate models. Current mass-univariate models 
assume uniform connectivity from input to every voxel; yet
this is obviously not the case. While
violating linearity assumptions or Gaussian assumptions 
requires us to be more conservative, the lack of any method
to switch brain circuits on and off means that any model
we make for brain regions would not be Turing complete;
in effect implying that humans are incapable of 
thought \citep{turing}.

So called Dynamic Causal Modeling is the beginning of 
the next phase of neurology. DCM is the first 
brain study to show any significant connection between 
diffusion tensor imaging and actual function layout in
the human brain \citep{dti_dcm}. By incorporating connections
between regions and a realistic activation model, DCM
corrects two of the largest problems with the GLM. While
there are other techniques that may be useful in the future,
they either lack physiological analogs (Artificial Neural
Networks) or are extremely computationally expensive 
(mutual information).  Given that much of the 
potential of the GLM has been exhausted, and that DCM 
is one of the first well defined methods capable of learning 
complicated brain circuits DCM is crucial to the future
of FMRI and our understanding of the human brain.

\section*{Results}
The particle filter shows great promise in being able 
to learn a variety of different regional activation
parameters. We have performed tests with a random binary
pulse train as input to a simulated region and then tested
the particle filter's ability to estimate the signal
parameters. Additionally random gaussian noise with an
Signal-To-Noise ratio of 2.0 was added. Figure 1 shows
the pulse train, the clean signal and the noisy signal, which
was the input to the particle filter. A random set of 
system parameters was chosen to simulate the region,
which the particle filter had no knowledge of other than
a mean and rough variance.
The tests were performed using 10,000 particles and took
approximately 4 minutes to compute.

%figure 1
\begin{figure}
\includegraphics[width=8in,height=4in]{noise.png}
\caption{Simulated time-series for a single region. The red line
shows the input stimulus, the blue line shows the base BOLD 
response, and the green line is the BOLD response with Gaussian
white noise added at SNR of 2}
\label{fig:noise}
\end{figure}

%figure 2
\begin{figure}
\includegraphics[width=8in,height=4in]{bold.png}
\caption{Particle Filter Results. The blue line is the "true"
(simulated) BOLD response, the red line
is the output of the particle filter with 2 standard deviations
in green. Note that the error bar for time 0 is outside the scope
of the image and is approximately $\pm .003$.}
\label{fig:bold}
\end{figure}

%figure 3
\begin{figure}
\includegraphics[width=8in,height=4in]{state.png}
\caption{Particle Filter Results. The blue line is the "true"
parameter and the green line is the estimated value for the parameter.
Note that the timescale for $S_t$ is different to highlight an typical
stimulus response. Again 2 standard deviations are shown.}
\label{fig:state}
\end{figure}

Figure 2 shows the estimated BOLD response versus the simulated
data (without noise). It is clear that as the estimated BOLD
response converges to the true BOLD as time progresses. This
is because more and more potential states are eliminated by system's
response to new input sequences. Convergence depends highly on 
the input signal and the weighting function thus choice of both
are extremely important. We have found that an impulse train
(emulated with .1 second width square waves) gives a very good
response. It is also important to choose an input with
some longer hold times, at least as long as than the 
expected time constants, which in this case was around 
8 seconds. The weighting function we have chosen is based
on the exponential distribution, with a variance equal to
the RMS of the signal. While it is important to converge
by the end of the time-series, it is often the case that 
converging too fast will harm the state estimation by over-
emphasizing the measurements of single time points. The rate
of convergence is well within control of the user of the 
particle filter based on the weighting function and the frequency
of re-sampling. Note that resampling can cause quantization
errors due to the nature of estimating a long tailed 
distribution with a finite number of samples. 

Figure 3 shows the estimated versus simulated values for
several key parameters. Notice how the variance drops as
the particle filter continues. The parameters shown are
$V_0$ which is a scaling parameter, $\tau_s$, and $\tau_f$
which are both time constants and $s_t$ which is a hidden
state variable, estimating the flow inducing signal. Note that
even though some of the parameters don't converge to the exact
value, that the estimated $s_t$ still matches the true
$s_t$ relatively well. It is often the cast that although a
few parameters don't converge to their true value, one 
parameter may pick up the slack for another. It is of
note that $\tau_s$ and $\tau_f$ do converge at least close
to their true values, because they cannot be determined
from steady state. 

\bibliographystyle{abbrvnat}
\bibliography{proposal_micah}


\end{document}
