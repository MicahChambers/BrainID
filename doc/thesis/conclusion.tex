\chapter{Conclusion}
\label{sec:Conclusion}
This work has proposed and demonstrated the use of a particle filter
for the estimation of BOLD model parameters. Because many of the parameters
were under constrained from the point of view of the BOLD response, a
full posterior estimate was a more logical approach to parameter estimation
than typical point estimates of parameters. The particle filter also converged
quickly to correct output estimates, and thus provides a framework
for future real-time FMRI experiments. Final BOLD estimates were also comparable
to the results of SPM. 

% Overview of results
%% Parameters under-constrained
%% Output estimates good
\section{Review of Results}
The results unequivocally show that the parameters of the BOLD model are under
constrained. While unsurprising given the sensitivity analysis in \cite{Deneux2006},
it is still an important limitation when calculating parameters. Thus, other methods
that depend on a single point estimate of the parameters, such as kalman filters
or even least squares are limited to estimating the BOLD signal, but the estimate
of the underlying parameters and state variables are suspect at best. Similarly, as the
histograms in \autoref{sec:Multi-voxel Simulation} and \autoref{sec:Real Data Parameter Estimates}
show, using the mean to estimate parameters also makes little sense in the particle filter
case. Attributing the variance of estimate to the underlying parameter distribution would
also be a mistake. While it is definitely true that parameters vary from person to person
and region to region, the estimated distributions in \autoref{sec:Real Data Parameter Estimates} 
are smoothed versions of those distributions. For this reason, any analysis of parameters should
take the entire estimated posterior distribution into account, and perform F-tests on that
distribution, rather than an idealized Gaussian. In this sense, particle filters represent
an important step forward in BOLD parameter estimation. It is now clear that representing
the uncertainty in parameters as a Gaussian is insufficient; so using a particle filter
or Bayesian estimate of the posterior is not simply an enhancement, but a necessary precaution.

Although point estimates of parameters are not dependable estimates of the 
true parameters, they are by no means useless. Analysis of differences in parameters
could be medically relevant. Additionally, a single estimate for parameters is still able
to form a good estimate of the BOLD signal. This factor forms a bridge to 
earlier types of activation tests, such as the statistical
parametric mapping. As the results in \autoref{sec:RealData} show, the heatmaps, especially
those of mutual information closely resembled the results of SPM. While the activation
tests were more sensitive, they were also more prone to false positive. However, it is
worth noting that certain enhancements could be useful in reducing false positives; for instance
by using the maximum likelihood of the final distribution, or even the median. The 
usefulness of these techniques depend on the degree to which the algorithm converged, 
and thus on the experimental design. Future work may shed further light on these
techniques. 

% Pros/Cons
%% Cons
%%% Computation longer than SPM
%%% Harder to interpret
%% Pro
%%% Non-parametric
%%% Real time
%%% Full Posterior
%%% More intuitive
\section{Review of Particle Filter Approach}
The Particle Filter algorithm was originally designed for on-line parameter 
estimation. For this reason, there is no guarantee of optimality or even 
convergence for finite measurements. However, for the BOLD nonlinear ODE 
this is less of a concern than it might first appear to be. For this
particular problem there can be no guarantee of a global minimum, and although
other techniques guarantee a local minimum, the particle filter doesn't settle 
to one of these local minima because it can settle into multiple ones. This
difference typifies the major difference between the particle filter and 
competing approaches. 

One difficulty
with the use of a particle filter with finite data is the calculation of a good
weight function $P(y_k | x_k)$  that will converge at a decent rate. If the weight
function does not sufficiently differentiate particles, the final distribution will 
no be significantly different from the prior distribution. On the other hand, if the 
weight function is too thin, it will unfairly eliminate viable particles. Given sufficient
measurements it is better to let the algorithm take longer to converge, because the
convergence will be better (more accurate). The particle filter takes longer
to run than Volterra approximation method from \autoref{sec:Background Linear Approximation};
however, it is free from the uncertainty of whether a quadratic approximation is 
sufficient for the BOLD model. 

The particle filter certainly has significant advantages over other estimation procedures
discussed in \autoref{sec:Prior Works}. The most important advantage is that it provides
an estimate of the posterior probability, rather than a single estimate. While it is natural
to want a simple estimate of parameters, such an estimate is impossible with this particular
model. The results are more difficult to interpret, but this is a necessity. The fact that 
the final distribution is not dependent on any particular distribution is also advantageous.
Of particular note; the final distribution does not need to conform to any parametric
distribution. While the particle filter was not fast for full brain calculations, its speed
was sufficient on a quad core machine to perform real time calculations of small regions
(approximate run time .27 seconds per voxel-measurement). Today it would be possible
 to perform real time analysis of 10 voxels on an average quad core. The algorithm also scales
well and does not require burdensome amounts of memory (approximately 11 megabytes). 
For this reason this algorithm is perfect for extension to vector or video card
based processors. 

A more practical benefit with the particle filter is that it is mathematically
simple. An understanding of Bayesian statistics is all that is necessary to understand
how the particle filter works. This is in contrast to the Volterra based approach
of \cite{Friston2000}, which is quite complex. Additionally very few assumptions
are needed for the particle filter. It is a common problem in traditional statistics 
for assumptions to be unrealistic which means the results may be invalid or at 
least in question. 
Fewer and more realistic assumptions mean that the particle filter is more robust
to unforeseen difficulties in FMRI data. 

% Enhancements/Dehancements 
%% flatten prior
%% \% difference using spline?
%% "smart knots"
%% DC gain as a parameter
%% linearizing
\section{Particle Filter Variations}
There are quite a few areas where the particle filter could be tuned. While
some of these areas were touched on \autoref{sec:Methods}, certain parts were
not used in the analysis and so weren't mentioned. 

\subsection{Prior Distribution}
Starting with a flattened
prior (by setting weights to be non-uniform after the initial particles have
been drawn) was not used in the final analysis. The reason for not flattening
the prior was a practical issue with weighting points in a 
7 dimensional joint distribution. Often differences in particle weights approached machine 
precision, meaning that the flattened prior was actually far from flat. Instead
the distribution became unpredictable, which made the results unpredictable. This
is quantization issue that will exist unless the prior were made to be deterministic
or the initial number particles was raised to computationally prohibitive levels.

Wider prior

\subsection{Detrending}
A large number of de-trending methods were tested, but all had some major problem. 
One viable method that could be utilized given the right experimental design is detrending
based on areas of low activity. This has the advantage that it wouldn't require an arbitrary
constant to be added to the pre-processed signal for the BOLD model to fit properly.
The disadvantage of this approach is that it could hide long fall times by normalizing them
out. It also requires periodic breaks in the stimulus, which could reduce value of those
samples for fitting purposes. 

Another potential way to deal with detrending is to 
linearize. This would use the delta between measurements for fitting rather than the 
direct value. This has the advantage of not requiring detrending and thus 
gives nearly raw data to the particle filter
for processing. The effectiveness of this method depends directly on the type of stimulus.
In an event driven stimulus with sparse stimuli this could be extremely effective because
the DC level has minimal data; however the longer high levels are held the less effective
this will be. On the other hand, splines will also reduce the plateaus, so its possible
linearizing may be just as effective. I found in tests that large drift-low white noise
type signals performed much better with a linearization approach, as one might expect. 
The results were equal to or worse in real data; but that was only for one particular
stimulus sequence. 

A subtle difference that could be made in detrending is how the percent difference
is calculated. In this work the spline was subtracted, and then the result
was divided by the average of the initial signal. A more correct method may be dividing
by the spline value at that particular point. The reason for not dividing by the spline
at that point is that it could be less stable, and in a sense the trend has already
been removed. The difference is not particularly large though 
(dividing by 1020 rather than 1000 for instance) as long as the spline didn't have heavy swings. 

Finally, rather than adding a constant to the detrended data before
applying the particle filter, a DC gain parameter could be added to the model. In
tests, this could work well, however, it often did not. The problem with this
could simply be the addition of another degree of freedom without any increase in
the number of particles. Increasing the number of particles further though
can be computationally intractable. Thus, while this is in a sense the correct
solution, it is an impractical one. 


% Future works
%% Extensive test of quality of volterra kernel estimate from friston
%% Better de-trending
%% Automatic estimation of measurement error.
%% Using s as the input to other regions. 
%% Extensive parameter estimation to find a correct prior
%% Increased constraints on parameters using physiological data
%% Incorporating Blood Flow/Volume data as separate measurement
\section{Future Works}

A limitation often reached in previous works was an inconsistency of
parameter estimates, most likely because of covariance between the model parameters.
Although individual studies got consistent results, those results often differed
widely from other similar studies. The reason for this is rather clear from the
simulation results in \autoref{sec:SimLowNoise}. There is a significant amount of
trade off between parameters to the point that a signal set of parameters
is most likely not possible to derive from the BOLD response alone. It
will therefore be beneficial to combine BOLD studies with cerebral blood
flow or cerebral blood volume studies to gain multiple more measurements and
further constrain the model. That said, the benefit of the particle filter
is that it provides a full posterior distribution at the final time step.
As such the true solution should be encoded in the particle filter's final 
distribution given priors that encompass the true parameters. This is beneficial
in two ways; first, if, after the fact, some parameter becomes known 
from outside observation, it is then possible to construct a new probability
conditional on the new observation. Secondly the results from multiple runs
may be reasonably concatenated, using the final distribution from the previous
run as the prior distribution of the next run. Rather than simply providing a 
staring point for parameters to converge from, it in fact continues convergence
from the previous stopping point. 

Although many versions of the BOLD model exist, and it is tempting to use 
more detailed models; from the results found here the issue of bias error
from the BOLD model is not the biggest concern. Clarifying the distributions
of parameters for the prior should be the first concern; currently no multi-patient
full-volume studies have been done to estimate parameters.
One future study that would be beneficial in this way  would
be an extensive study of what the priors should truly be. Although \cite{Friston2000}
gives an estimate of what is thought to be reasonable values, and later studies
published their estimate of the distributions, given the interplay between
parameters it is unlikely that these priors are true to actual distribution
that occurs in vivo. As I mentioned previously, the addition of simultaneous
flow or volume measurements are another potentially powerful way to further confine the model,
and thus deal with the elasticity of parameters. As I mentioned in 
\autoref{sec:BackgroundConclusion}, a chief advantage of using physiologically
plausible models is that such data may in fact be added with relative ease.

Automatic detection of the noise level in the signal, to get a decent wieghting
function. Large scale activation to get more parameter estimates. 
Viscoelastic effects from \cite{Buxton2004}, discussed in \cite{Deneux2006}.

In conclusion, using particle filters to estimate the BOLD response are 
a powerful method of fitting to noisy data. The technique also
holds great promise as extensible platform to build more advanced models
and techniques on top of. Integrating further information is necessary to
move beyond the traditional Statistical Parametric Mapping and moving
toward biologically and medically relevant FMRI scanning techniques. 
