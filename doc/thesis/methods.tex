\chapter{Methods}
\section{Choosing $\hat{Pr}(y(T) | x(T))$}
Choosing a representation of an unknown distribution is certainly tricky,
and so the fact that $\hat{Pr}(y(T) | x(T)) = \nu_d + \nu_y$ means that
there is a significant piece of the algorithm that is based primarily conjecture.
Studies of the noise in FMRI typically attribute noise to a Gaussian random
variable or an additive noise process with Gaussian steps. 

\subsection{Classical De-trending}
The non-stationary
aspect of a Weiner process as with $\nu_d$ is difficult to compensate for, and so various methods
have been developed to compensate for it. \cite{Tanabe2002} and \cite{Smith1999} have
demonstrated that this component is prevalent, and may in fact be a characteristic
of FMRI. In some studies, as many as half the voxels benefit from detrending, meaning
that this is certainly a serious barrier to inference. All the existing methods are performed
during the preprocessing stage, rather than as an integral part of analyzing the BOLD
signal. There is no shortage of theories on the "best" method of detrending; however
a head to head comparison, \cite{Tanabe2002}, showed that in most cases subtracting off
a spline works the best. The benefit of the spline versus wavelets, high pass 
filtering or other DC removal techniques is that the frequency response is not set.
Rather, the spline is adaptive to the input, having a low cut off if the signal's 
median stays constant but a high cut off frequency if the signal's median tends to
shift heavily over the course of time. In spite of this, the spline will still remove some
amount of signal, just like all of these methods, which is why the the method proposed
in \autoref{sec:Methods Delta Based Inference} was considered. 

When using the median-based spline techniques, the resulting signal is will almost
certainly end with a median of zero; in other words half the signal will be above zero,
half below. The problem with this is this is not the "natural" BOLD signal. More specifically,
when the signal is inactive, the BOLD response \emph{should} be at 0\% change from
the base level; activation may then increase, or for short periods decrease from this base.
After removing the spline, the BOLD resting state will be below 0\%.
This is troublesome because it reduces the power the BOLD model. One obvious solution
is to add an arbitrary constant to each BOLD response. Of course this won't scale well
to whole brain analysis. A more effective technique is adding a DC gain to the BOLD
model. Essentially every measurement would then have a constant level added to it. 
Like all the other model parameters, enough measurements should allow the correct
value to be found. On the downside adding another dimension increases the
complexity of the model, for a variable that is relatively obvious to a human.

Thus, a more conventional approach was used. A robust estimator of scale, was used
to determine roughly where the base level was. The Median Absolute Deviation (MAD)
proved to be extremely accurate in determining how much to shift the signal up
by. We tested both methods during the course of analysis, and found that the increase 
in model complexity far outweighted the slight increase in flexibility. Its 
possible that a more accurate method may exist; however, for this case the 
MAD seemed to work perfectly, as \autoref{fig:DesplineQuality} shows. 

\begin{figure}
\caption{image of de-spline'd lines with "true" lines}
\label{fig:DesplineQuality}
\end{figure}

\subsection{Delta Based Inference}
\label{sec:Methods Delta Based Inference}
I also propose and test a different method of dealing with the so called "drift". 
Instead of comparing the direct output of the particle filter with the direct
measurement, the algorith compares the change in signal over a single TR. 
In most signal processing cases this would foolish, but that is because the 
general assumption that all noise is high frequency is not the case here.
In fact, every pipeline for the analysis of BOLD signal uses a high pass filter,
but low poss filters are rarely applied, because it is a well known fact that 
most of the signal is in the high frequency range and most of the noise is actually 
in the low frequency range. The particle filter is an 
extremely robust method of inference, and so I would assert that the particle
filter ought to be given as \emph{raw} data as possible. While taking direct measurements
without de-trending would give awful results, using the difference removes the 
DC component and turns a Weiner process into a Gaussian random variable. 

\begin{equation}
\Delta y = y(t) - y(t-1) = g(x(t)) - g(x(t-1)) + \nu_y(t) - \nu_y(t-1) + \nu_d(t) - \nu(t-1)
\label{eq:measass_delta}
\end{equation}

Because $\nu_d$ is a Weiner process, then $\nu_d(t) - \nu_d(t-1)$ is simply a Gaussian step. If
$\nu_d$ is some other additive process, the difference will still be one of a few stable
distributions. If $\nu_y$ is i.i.d. then the resulting distribution will still be zero mean
with a maximum variance of twice the original variance. All the assumptions made originally
for the particle filter hold, and all of the parameters may be distinguished based on
the step sizes, thus it is not unreasonable to attempt to match the string of step sizes
rather than string of direct readings. 

\begin{figure}
\label{fig:FrequencyGraphs}
\caption{frequency response graphs, highlighting noise frequency range and signal frequency range}
\end{figure}

\section{Preprocessing}
\label{sec:Methods Preprocessing}
As discussed in the section on de-trending, the normal pipeline for analyzing
FMRI involves a great deal of preprocessing. In this paper we make an effort to
minimize any type of preprocessing that will degrade the signal. 
After FMRI data has been acquired it is always necessary to modify the
data in some way to make different runs comparable. Because FMRI signal
levels are unit-less, at the very least it is necessary to convert
the data into \% difference from the baseline. This process removes no data
from signal since it merely subtracting then dividing by a constant. This
is the signal that was input into the delta based particle filter.
Of course there are much more advanced ways of performing this task.
The generally accepted standard is actually to use a high pass filter, although the
cutoff frequency is application dependent and often applied haphazardly.
The high pass filter thus removes the DC component of the signal, and 
some amount of the so called "drift". The problem with this method is that it is
not adaptive to the input. Huge variations in drift frequencies can exist 
in a single time-series. Thus, a single cutoff frequency could miss a significant
drift component, or it could remove \emph{actual} signal, if the cutoff frequency is
set too high. This is why, as I mentioned in the De-trending section, a spline
based detrending method will generally give better results. 

For simulated and real images (tests with multiple time-series), tests were 
also run with and without Gaussian filtering with sigma of %not sure todo
were run, since it is standard
practice to apply a Gaussian spatial filter to the images at each timestep. Obviously
a spatial filter such as Gaussian filtering increased SNR but can also lead to less
precision in the output maps.

\section{Simulation}
We performed two types of simulations. First, we simulated a single BOLD time-series based
on a random chosen set of model parameters. This process was relatively straight forward
given the state-space equations for the BOLD signal. After a "true" signal was generated,
we then added a carrier level, since BOLD is typically measured as a \% difference from the
base level. Finally, we added Gaussian noise, and a Weiner Process to the clean signal. The
variance of the Gaussian noise may be expressed in terms of the desired noise SNR, $R$ as:
\begin{equation}
var(y_{noisy}) = var(y) / R
\end{equation}
Since SNR doesn't have quite the same meaning for a Wiener process based noise, the variance 
of the Gaussian steps was set to be:
\begin{equation}
var(y_{noisy}) = var(y) / (4R)
\end{equation}
Once this noisy simulated time series was generated, the exact same particle filter algorithm
that would later be run on full sized images, was run on this single voxel image. We ran
a series of tests to determine the convergence rate of the particle filter, the number
of particles that were required, how weighting functions compared, how different de-trending
methods compared with each other and, finally the variance of the result. By running the exact
time-series with different noise realizations, it was possible to determine the model variance.
As the reader may know, the error of an estimator may be calculated as:
\begin{equation}
MSE(\Theta) = Var(\Theta) + Bias(\Theta)^2
\end{equation}
The variance is an expression of how much the result would change for different noise realizations,
whereas the bias is an expression of how well the model matches the true underlying model. In
this case, because the same model is being used in the particle filter and underlying simulation,
the bias is actually zero. Obviously when this is calculated using \emph{real} data with an unknown
underlying state space equation, there will be some amount of bias error, but assuming that the
noise is similar to the noise used in these tests, the model variance will actually be about the
same. Thus calculating the model variance is extremely helpful in calculating how well determined
our model is, and how consistent it will be for real data. A single timeseries, as opposed to the
thousands present in a real image, makes it easier to 
compare the output with the ground truth, with various parameters. 

Second we used a modified version of the FSL tool 
POSSUM to generate an entire FMRI image from a parameter map. The parameter map was generated
by creating a random image, smoothing it with a large Gaussian kernel, then thresholding
the results. Finally connected regions were each given a set of parameters from a finite
list of randomly chosen parameter sets. The result was a four dimensional (length x width
x height x parameter) image with spatially varying parameters. Time-series of activation level
was generated for each set of parameters, then activation levels were fed into POSSUM's 
function for generating frequency domain data. The patche for POSSUM will be made available.
For each time-series in the simulate FMRI image, the final \emph{static} parameters are saved
into a parameter map. This parameter map may then be compared to the map used to generate the 
simulated data; additionally a new simulation using the calculated parameters may also be 
generated to test the functional difference between the two maps. This would give an absolute 
quantitative difference between the two parameter sets irrespective to parameter slopiness.
So for instance, if $V_0$ is halved, $\epsilon$ doubling may very well give a similar result.
In this case the \% difference between the parameters will be large in each case, but the functional
difference between the parameters will not be great. This is obviously a bad situation, which
is why we wanted to test for it.

\section{Real Data}
Finally, we also performed inference based on real FMRI data. The scanner we used...
%... more specifics...

The final result from calculating parameters with the real data was similar to that
from the results from the POSSUM simulated data. The difference being that there was
no ground truth the check it with. 

