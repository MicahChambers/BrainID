\chapter{Methods}
Although the particle filter I used for this work is a standard Regularized
Particle filter, as described in \cite{Arulampalam2002a}, optimizing the 
particle filter for use with FMRI data is non-trivial. 

\section{Prior Distribution}
For the BOLD model described in \autoref{sec:BOLD Physiology}, several
different studies have endeavored to calculate parameters. The results
of these studies may be found in \autoref{tab:Params}, and the methods 
used for each may be found in \autoref{sec:Prior Works}. Unfortunately,
\cite{Friston2000} only studied small regions; and most research with
reasonable speed (including \cite{Friston2002b}) used these results as 
the source for their priors. 
The one exception is \cite{Johnston2008}, which came to an extremely different
set of distributions. For a particle filter, the choice of a prior is
the single most important design choice. Consequently for this work,
I wanted to be conservative; which meant going with the accepted result,
\cite{Friston2000}. This constrains the usefulness of the model to
areas that fall within the prior distribution, yet will allow results
to be comparable to other works. There is a significant need for better
estimates of the physiological parameters; and, while physical experiments
may not be possible, it would not be unreasonable to do a study with
exhaustive simulated annealing or hill climbing tests for multiple
regions and multiple patients. The purpose for this work is to determine
the fitness of particle filters for this task; because with good 
priors it can be extremely fast. Ultimately this algorithm may become
more useful as more studies are done on BOLD model that don't require
priors. 

Therefore, because \cite{Friston2000} represents the current best
knowledge about the BOLD parameters, it forms the basis
for the prior distributions in this paper. 

\begin{table}[t]
\centering
\begin{tabular}{|c || c | c |}
\hline 
Parameter & Distribution & $\mu$ & $\sigma$ \\
\hline
$\tau_0$ & Gamma & .98 & .5  \\
$\alpha$ & Gamma & .33 & .045\\
$E_0$    & Gamma & .34 & .03  \\
$V_0$    & Gamma & .03 & .006 \\
$\tau_s$ & Gamma & 1.54  & .5 \\
$\tau_f$ & Gamma & 2.46  & .5 \\
$\epsilon$ & Gamma & .64 & .4 \\
\hline
\end{tabular}
\caption{}
\label{tab:Prior} 
\end{table}

Note that although the mean remains the same for all the 
parameters other than $\epsilon$, the standard deviation is set
much higher to account for the disagreement between studies
(\autoref{tab:Params}). 
Because all the parameters are taken to be strictly positive, and the
standard deviations are approaching the mean, I used a gamma distribution.
This prevents the Gaussian from placing parameters in the nonsensical 
territory of negative activation, or negative time constants.

\section{Model}
As originally written in \autoref{sec:BOLD Physiology} the state variables
for the BOLD model are as follows:
\begin{eqnarray}
\dot{s} &=& \epsilon u(t) - \frac{s}{\tau_s} - \frac{f - 1}{\tau_f} \\
\dot{f} &=& s\\
\dot{v} &=& \frac{1}{\tau_0}(f - v^\alpha)\\
\dot{q} &=& \frac{1}{\tau_0}(\frac{f(1-(1-E_0)^f)}{E_0} - \frac{q}{v^{1-1/\alpha}})
\end{eqnarray}
The original assumption regarding particle filter models (\autoref{sec:Particle Filter Model})
included noise in the update of $x$, however that is not included here.
The reason for the difference is that cloud of particles is, to some extent,
able to model that noise. It is common, however, to actually model that noise
in particle filters by adding a random value to each updated state variable. 
Because the purpose of this particle filter is to learn the underlying distribution
of the static parameters, rather than precisely model the time course of the 
in the dynamic parameters ($\{s,f,v,q\}$) this noise is left out. Because the
BOLD model is dissipative, when no stimuli are applied, all the particles 
will decay to ($\{0,1,1,1\}$) which they should. If evolution noise were added, the 
particles would then weight particles based on the results of that noise, rather
than on the quality of the static parameters. Typical particle filter uses
also use this state noise as an exploratory measure; however given the 
high dimensionality of the system, a vast number of particles would be 
necessary for this to come to fruition. The noise in the BOLD signal is such
that several particles all tend to be equally correct; the particle filter will
still leave variance in the solution; which turned out to be the case.

Thus, at each time step, the states were linearized, and each state variable,
$s$, $f$, $v$, $q$, were changed according their previous rate of change.
Because of the difficulty of solving a system of nonlinear equations, I 
did not use the typical Runge-Kutta 4/5 technique to optimize step sizes. 
The cost of missing a feature in these differential equations typically leads
to non-real numbers shortly down the road. Because the non-real numbers do 
not come until the solution has had the chance to update two or three more
times, taking long steps can result in catastrophic and un-recoverable errors. 
Typically a step size of .001 was used, after finding that even .01 can
at times lead to the state equations careening out of control.

\section{Resampling}
The algorithm for resampling is described in \autoref{sec:Particle Filter Resampling}.
The primary design decision for resampling is the regularizing
kernel. As mentioned previously, the Gaussian kernel is extremely convenient,
because it is so easy to sample from. As discussed in \autoref{sec:Particle Filter Resampling},
as long as resampling is kept as a last resort, some amount of over-smoothing
won't impair convergence. Therefore, for this work I chose a Gaussian kernel of
bandwidth equal to the original distribution's covariance. Obviously this will
apply a rather large amount of smoothing to the distribution; however, on average
resampling is only applied every 10 to 20 measurements, and because randomization
is being applied to model updates this gives the filter some mobility. 

The question of how often to resample is complex. The benefit of resampling
is that particles that were minimally important can give much more 
granularity in the middle of the distribution. At the same time, resampling
right after a batch of outliers can result in an incorrect distribution.
Because the memory of particle weights is not particularly long, "good" particles
could be removed by resampling very often. Additionally, because regularized
resampling is being with a large bandwidth, I did not want the particle filter
to be prevented from converging by resampling too often. Therefore, only
when the effective sample size dropped below 25 for two measurement points
in a row did I resample. This seemed to give good results, and avoided 
temporary drops in the $N_{eff}$ caused by outliers. 

\section{Choosing ${Pr}(y(T) | x(T))$}
Choosing a representation of an unknown distribution is certainly tricky,
and so the fact that $\hat{Pr}(y(T) | x(T)) = \nu_d + \nu_y$ means that
there is a significant piece of the algorithm that is based primarily conjecture.
Studies of the noise in FMRI typically attribute noise to a Gaussian random
variable or an additive noise process with Gaussian steps. 

Another natural choice might be one of the robust estimator weight functions, for
example the Huber or bi-square. For the purpose of this work we will stick
with long tailed distributions, however it is worth noting that long tails
may not be the optimal choice in all, or even this situation. The justification
for long tailed distributions is that we believe the noise to be long tailed,
and the variance of the noise is not well known.

Therefore, we tried three weighting functions based on three distributions: Gaussian, 
Laplace and the Cauchy. The standard deviation of the distribution is extremely
important to the convergence of particle filter. A standard deviation that is 
too large will not allow the distribution to converge in any reasonable number of 
measurements. A standard deviation below the standard deviation of the noise 
will cause the algorithm to throw out perfectly acceptable particles. 
The weighting function ultimately will shape the output distribution, $P[y]$, into that
distribution, however the distribution of $x$ will still approach a reasonable
estimate of its true distribution. Even an overly wide weighting function, will
allow the Gaussian Mixture estimate of $X$ to converge to the correct location 
parameters of the "real" posterior distribution, though the scale parameters may 
be overly large.

A reasonable method of setting the standard deviation of $\Omega$ may be by 
taking a small sample from "resting" data and using the sample standard deviation.
Since this is the first attempt at using particle filters for modeling the 
BOLD model, in this work we set the standard deviation manually at <weight standard dev>,
because it gives a more consistency and control. Of course this could be taken
further, by testing the sample data against a set of stock distributions
and choosing the best fit. This of course depends on having enough samples
to make a reasonable inference, which may not always exist. 


\subsection{Classical De-trending}
The non-stationary
aspect of a Weiner process as with $\nu_d$ is difficult to compensate for, and so various methods
have been developed to compensate for it. \cite{Tanabe2002} and \cite{Smith1999} have
demonstrated that this component is prevalent, and may in fact be a characteristic
of FMRI. In some studies, as many as half the voxels benefit from detrending, meaning
that this is certainly a serious barrier to inference. All the existing methods are performed
during the preprocessing stage, rather than as an integral part of analyzing the BOLD
signal. There is no shortage of theories on the "best" method of detrending; however
a head to head comparison, \cite{Tanabe2002}, showed that in most cases subtracting off
a spline works the best. The benefit of the spline versus wavelets, high pass 
filtering or other DC removal techniques is that the frequency response is not set.
Rather, the spline is adaptive to the input, having a low cut off if the signal's 
median stays constant but a high cut off frequency if the signal's median tends to
shift heavily over the course of time. In spite of this, the spline will still remove some
amount of signal, just like all of these methods, which is why the the method proposed
in \autoref{sec:Methods Delta Based Inference} was considered. 

When using the median-based spline techniques, the resulting signal is will almost
certainly end with a median of zero; in other words half the signal will be above zero,
half below. The problem with this is this is not the "natural" BOLD signal. More specifically,
when the signal is inactive, the BOLD response \emph{should} be at 0\% change from
the base level; activation may then increase, or for short periods decrease from this base.
After removing the spline, the BOLD resting state will be below 0\%.
This is troublesome because it reduces the power the BOLD model. One obvious solution
is to add an arbitrary constant to each BOLD response. Of course this won't scale well
to whole brain analysis. A more effective technique is adding a DC gain to the BOLD
model. Essentially every measurement would then have a constant level added to it. 
Like all the other model parameters, enough measurements should allow the correct
value to be found. On the downside adding another dimension increases the
complexity of the model, for a variable that is relatively obvious to a human.

Thus, a more conventional approach was used. A robust estimator of scale, was used
to determine roughly where the base level was. The Median Absolute Deviation (MAD)
proved to be extremely accurate in determining how much to shift the signal up
by. We tested both methods during the course of analysis, and found that the increase 
in model complexity far outweighted the slight increase in flexibility. Its 
possible that a more accurate method may exist; however, for this case the 
MAD seemed to work perfectly, as \autoref{fig:DesplineQuality} shows. 

\begin{figure}
\caption{image of de-spline'd lines with "true" lines}
\label{fig:DesplineQuality}
\end{figure}

\subsection{Delta Based Inference}
\label{sec:Methods Delta Based Inference}
I also propose and test a different method of dealing with the so called "drift". 
Instead of comparing the direct output of the particle filter with the direct
measurement, the algorith compares the change in signal over a single TR. 
In most signal processing cases this would foolish, but that is because the 
general assumption that all noise is high frequency is not the case here.
In fact, every pipeline for the analysis of BOLD signal uses a high pass filter,
but low poss filters are rarely applied, because it is a well known fact that 
most of the signal is in the high frequency range and most of the noise is actually 
in the low frequency range. The particle filter is an 
extremely robust method of inference, and so I would assert that the particle
filter ought to be given as \emph{raw} data as possible. While taking direct measurements
without de-trending would give awful results, using the difference removes the 
DC component and turns a Weiner process into a Gaussian random variable. 

\begin{equation}
\Delta y = y(t) - y(t-1) = g(x(t)) - g(x(t-1)) + \nu_y(t) - \nu_y(t-1) + \nu_d(t) - \nu(t-1)
\label{eq:measass_delta}
\end{equation}

Because $\nu_d$ is a Weiner process, then $\nu_d(t) - \nu_d(t-1)$ is simply a Gaussian step. If
$\nu_d$ is some other additive process, the difference will still be one of a few stable
distributions. If $\nu_y$ is i.i.d. then the resulting distribution will still be zero mean
with a maximum variance of twice the original variance. All the assumptions made originally
for the particle filter hold, and all of the parameters may be distinguished based on
the step sizes, thus it is not unreasonable to attempt to match the string of step sizes
rather than string of direct readings. 

\begin{figure}
\label{fig:FrequencyGraphs}
\caption{frequency response graphs, highlighting noise frequency range and signal frequency range}
\end{figure}

\section{Preprocessing}
\label{sec:Methods Preprocessing}
As discussed in the section on de-trending, the normal pipeline for analyzing
FMRI involves a great deal of preprocessing. In this paper we make an effort to
minimize any type of preprocessing that will degrade the signal. 
After FMRI data has been acquired it is always necessary to modify the
data in some way to make different runs comparable. Because FMRI signal
levels are unit-less, at the very least it is necessary to convert
the data into \% difference from the baseline. This process removes no data
from signal since it merely subtracting then dividing by a constant. This
is the signal that was input into the delta based particle filter.
Of course there are much more advanced ways of performing this task.
The generally accepted standard is actually to use a high pass filter, although the
cutoff frequency is application dependent and often applied haphazardly.
The high pass filter thus removes the DC component of the signal, and 
some amount of the so called "drift". The problem with this method is that it is
not adaptive to the input. Huge variations in drift frequencies can exist 
in a single time-series. Thus, a single cutoff frequency could miss a significant
drift component, or it could remove \emph{actual} signal, if the cutoff frequency is
set too high. This is why, as I mentioned in the De-trending section, a spline
based detrending method will generally give better results. 

For simulated and real images (tests with multiple time-series), tests were 
also run with and without Gaussian filtering with sigma of %not sure todo
were run, since it is standard
practice to apply a Gaussian spatial filter to the images at each timestep. Obviously
a spatial filter such as Gaussian filtering increased SNR but can also lead to less
precision in the output maps.

\section{Simulation}
We performed two types of simulations. First, we simulated a single BOLD time-series based
on a random chosen set of model parameters. This process was relatively straight forward
given the state-space equations for the BOLD signal. After a "true" signal was generated,
we then added a carrier level, since BOLD is typically measured as a \% difference from the
base level. Finally, we added Gaussian noise, and a Weiner Process to the clean signal. The
variance of the Gaussian noise may be expressed in terms of the desired noise SNR, $R$ as:
\begin{equation}
var(y_{noisy}) = var(y) / R
\end{equation}
Since SNR doesn't have quite the same meaning for a Wiener process based noise, the variance 
of the Gaussian steps was set to be:
\begin{equation}
var(y_{noisy}) = var(y) / (4R)
\end{equation}
Once this noisy simulated time series was generated, the exact same particle filter algorithm
that would later be run on full sized images, was run on this single voxel image. We ran
a series of tests to determine the convergence rate of the particle filter, the number
of particles that were required, how weighting functions compared, how different de-trending
methods compared with each other and, finally the variance of the result. By running the exact
time-series with different noise realizations, it was possible to determine the model variance.
As the reader may know, the error of an estimator may be calculated as:
\begin{equation}
MSE(\Theta) = Var(\Theta) + Bias(\Theta)^2
\end{equation}
The variance is an expression of how much the result would change for different noise realizations,
whereas the bias is an expression of how well the model matches the true underlying model. In
this case, because the same model is being used in the particle filter and underlying simulation,
the bias is actually zero. Obviously when this is calculated using \emph{real} data with an unknown
underlying state space equation, there will be some amount of bias error, but assuming that the
noise is similar to the noise used in these tests, the model variance will actually be about the
same. Thus calculating the model variance is extremely helpful in calculating how well determined
our model is, and how consistent it will be for real data. A single timeseries, as opposed to the
thousands present in a real image, makes it easier to 
compare the output with the ground truth, with various parameters. 

Second we used a modified version of the FSL tool 
POSSUM to generate an entire FMRI image from a parameter map. The parameter map was generated
by creating a random image, smoothing it with a large Gaussian kernel, then thresholding
the results. Finally connected regions were each given a set of parameters from a finite
list of randomly chosen parameter sets. The result was a four dimensional (length x width
x height x parameter) image with spatially varying parameters. Time-series of activation level
was generated for each set of parameters, then activation levels were fed into POSSUM's 
function for generating frequency domain data. The patche for POSSUM will be made available.
For each time-series in the simulate FMRI image, the final \emph{static} parameters are saved
into a parameter map. This parameter map may then be compared to the map used to generate the 
simulated data; additionally a new simulation using the calculated parameters may also be 
generated to test the functional difference between the two maps. This would give an absolute 
quantitative difference between the two parameter sets irrespective to parameter slopiness.
So for instance, if $V_0$ is halved, $\epsilon$ doubling may very well give a similar result.
In this case the \% difference between the parameters will be large in each case, but the functional
difference between the parameters will not be great. This is obviously a bad situation, which
is why we wanted to test for it.

\section{Real Data}
Finally, we also performed inference based on real FMRI data. The scanner we used...
%... more specifics...

The final result from calculating parameters with the real data was similar to that
from the results from the POSSUM simulated data. The difference being that there was
no ground truth the check it with. 

