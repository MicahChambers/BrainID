\chapter{Methods}
Although the particle filter I used for this work is a standard Regularized
Particle filter, as described in \cite{Arulampalam2002a}, optimizing the 
particle filter for use with FMRI data is non-trivial. 

\section{Prior Distribution}
For the BOLD model described in \autoref{sec:BOLD Physiology}, several
different studies have endeavored to calculate parameters. The results
of these studies may be found in \autoref{tab:Params}, and the methods 
used for each may be found in \autoref{sec:Prior Works}. Unfortunately,
\cite{Friston2000} only studied small regions; and most research with
reasonable speed (including \cite{Friston2002b}) used these results as 
the source for their priors. 
The one exception is \cite{Johnston2008}, which came to an extremely different
set of distributions. For a particle filter, the choice of a prior is
the single most important design choice. Consequently for this work,
I wanted to be conservative; which meant going with the accepted result,
\cite{Friston2000}. This constrains the usefulness of the model to
areas that fall within the prior distribution, yet will allow results
to be comparable to other works. There is a significant need for better
estimates of the physiological parameters; and, while physical experiments
may not be possible, it would not be unreasonable to do a study with
exhaustive simulated annealing or hill climbing tests for multiple
regions and multiple patients. The purpose for this work is to determine
the fitness of particle filters for this task; because with good 
priors it can be extremely fast. Ultimately this algorithm may become
more useful as more studies are done on BOLD model that don't require
priors. 

Therefore, because \cite{Friston2000} represents the current best
knowledge about the BOLD parameters, it forms the basis
for the prior distributions in this paper. 

\begin{table}[t]
\centering
\begin{tabular}{|c || c | c | c |}
\hline 
Parameter & Distribution & $\mu$ & $\sigma$ \\
\hline
$\tau_0$ & Gamma & .98 & .5  \\
$\alpha$ & Gamma & .33 & .045\\
$E_0$    & Gamma & .34 & .03  \\
$V_0$    & Gamma & .03 & .006 \\
$\tau_s$ & Gamma & 1.54  & .5 \\
$\tau_f$ & Gamma & 2.46  & .5 \\
$\epsilon$ & Gamma & .64 & .4 \\
\hline
\end{tabular}
\caption{}
\label{tab:Prior} 
\end{table}

Note that although the mean remains the same for all the 
parameters other than $\epsilon$, the standard deviation is set
much higher to account for the disagreement between studies
(\autoref{tab:Params}). 
Because all the parameters are taken to be strictly positive, and the
standard deviations are approaching the mean, I used a gamma distribution.
This prevents the Gaussian from placing parameters in the nonsensical 
territory of negative activation, or negative time constants.

Another aspect of the prior is using enough particles to get a 
sufficiently dense approximation of the prior. For 7 dimensions,
getting a dense prior is extremely difficult. Insufficiently
dense particles will result in inconsistent results, of course the
processing time will scale up directly with the number of particles.
A dense initial estimate is important so that some particles land
near the solution; but as the variance decreases the number of 
particles needed decreases as well. Thus, as a heuristic, initially
the number of particles is set to 28,000, but after resampling,
the number of particles is dropped to 1,000. Typically during the 
first few measurements the variance drops precipitously since most particles
don't describe the system well. The particles that are left are in a
much more compact location, allowing them to be estimated with 
significantly fewer particles. These numbers aren't set in stone,
and depending on the complexity of the system or desired accuracy
they could be changed; however, they seem to be the minimum that
will give consistent results.

\section{Model}
As originally written in \autoref{sec:BOLD Physiology} the state variables
for the BOLD model are as follows:
\begin{eqnarray}
\dot{s} &=& \epsilon u(t) - \frac{s}{\tau_s} - \frac{f - 1}{\tau_f} \\
\dot{f} &=& s\\
\dot{v} &=& \frac{1}{\tau_0}(f - v^\alpha)\\
\dot{q} &=& \frac{1}{\tau_0}(\frac{f(1-(1-E_0)^f)}{E_0} - \frac{q}{v^{1-1/\alpha}})
\end{eqnarray}
The original assumption regarding particle filter models (\autoref{sec:Particle Filter Model})
included noise in the update of $x$, however that is not included here.
The reason for the difference is that cloud of particles is, to some extent,
able to model that noise. It is common, however, to actually model that noise
in particle filters by adding a random value to each updated state variable. 
Because the purpose of this particle filter is to learn the underlying distribution
of the static parameters, rather than precisely model the time course of the 
in the dynamic parameters ($\{s,f,v,q\}$) this noise is left out. Because the
BOLD model is dissipative, when no stimuli are applied, all the particles 
will decay to ($\{0,1,1,1\}$) which they should. If evolution noise were added, the 
particles would then weight particles based on the results of that noise, rather
than on the quality of the static parameters. Typical particle filter uses
also use this state noise as an exploratory measure; however given the 
high dimensionality of the system, a vast number of particles would be 
necessary for this to come to fruition. The noise in the BOLD signal is such
that several particles all tend to be equally correct; the particle filter will
still leave variance in the solution; which turned out to be the case.

Thus, at each time step, the states were linearized, and each state variable,
$s$, $f$, $v$, $q$, were changed according their previous rate of change.
Because of the difficulty of solving a system of nonlinear equations, I 
did not use the typical Runge-Kutta 4/5 technique to optimize step sizes. 
The cost of missing a feature in these differential equations typically leads
to non-real numbers shortly down the road. Because the non-real numbers do 
not come until the solution has had the chance to update two or three more
times, taking long steps can result in catastrophic and un-recoverable errors. 
Typically a step size of .001 was used, after finding that even .01 can
at times lead to the state equations careening out of control.

\section{Resampling}
The algorithm for resampling is described in \autoref{sec:Particle Filter Resampling}.
The primary design decision for resampling is the regularizing
kernel. As mentioned previously, the Gaussian kernel is extremely convenient,
because it is very simple to sample from. As discussed in \autoref{sec:Particle Filter Resampling},
as long as resampling is kept as a last resort, some amount of over-smoothing
won't impair convergence. Therefore, for this work I chose a Gaussian kernel of
bandwidth equal to the original distribution's covariance. Obviously this will
apply a rather large amount of smoothing to the distribution; however, on average
resampling is only applied every 20 to 30 measurements, and because randomization
is being applied to model updates this gives the filter some mobility. 

Because the regularized resample will almost certainly over-smooth,
it is necessary to wait until the $N_{eff}$ drops relatively low
(say below 25) before resampling is performed. As an additional 
measure against sharp drops in the $N_{eff}$ that may quickly raise
back up, resampling is only performed when two consecutive low
$N_{eff}$'s are found. 
The danger in waiting longer to resample is that if there were no
particles in the vicinity of the solution, then particle deprivation
can occur.  When this happens, the resampling
stage will have an inappropriately small variance which will lead to an 
inappropriately small support for the distribution. After this, the 
distribution will be unrecoverable. Thus, when the $N_{eff}$ gets down
to extremely low values, usually this is an indication of particle
deprivation; something that is also usually accompanied by extremely
fast drops in variance. When this happens, it is preferable 
to return to a previous, wider estimate of scale for the regularization
and continue from there. Thus to keep particle deprivation from 
affecting the results; a previous covariance matrix is 
used as the regularization bandwidth. In my algorithm I use
the last covariance matrix for which the $E_{eff}$ was greater than
the threshold. This guarantees that there was at least some amount of 
particle diversity, and also helps prevent converging prematurely.

When some sort of particle deprivation has not occurred, regularized
resampling with a large bandwidth will slow down convergence. Thus
as mentioned previously, to prevent over smoothing, the particle
filter algorithm used here waits until 
the effective sample size drops below 25 for two measurement points
in a row. This seemed to give good results, and avoids
temporary drops in the $N_{eff}$ caused by outliers. 

\section{Choosing $P(y_k | x_k)$}
Choosing a representation of an unknown distribution tricky, and while
the usual choice is a Gaussian distribution, there are reasons why it
may not be the best.  Studies of the noise in FMRI typically characterise
noise as an additive noise process with Gaussian steps; a Wiener process. 

As noted in \autoref{sec:Introduction Noise}, the noise is not strictly Gaussian,
nor is it strictly Wiener. As with any unknown noise however, it is necessary 
to make some assumption. If the weighting function ($P(y_k | x_k)$) exactly
matches the measurement error, then the ideal particle filter will result.
Particles with $x_k$'s that estimate $y_k$ far out on the weighting
function will quickly have weights near 0. Thus, an weighting function that
exactly matches $P(Y(t) | X(t))$ will easily, and correctly throw out incorrect
particles. However assuming that this function is not going to be found 
for arbitrary voxels, it is necessary to choose a function to approximate this 
probability. The cost of choosing an overly broad distribution for this
function is an overly broad $P(x_k | y_k)$; and ultimately an overly
broad $P(x_k | y_{0:k})$. On the other hand, an overly thin distribution
will lead to, at best, an overly thin representation of these same 
distributions; but at wost will lead to particle deprivation (all particles
being zero-weighted). If the \emph{true} distribution were found, particle
deprivation would indicate that that time series is not being affected
in any way describable by the prior distribution/model. Particle deprivation
due to an under-variant weighting function gives no information. Thus,
the cost of under variance is significantly higher; a fact that definitely needs
to be taken into account. 

Because of the need to err on the side of caution I tested several weighting
functions. In addition to the Gaussian I also tested the Laplace and Cauchy
distributions, both of which have much wider tales than the Gaussian. The
benefit of the wider tailed distributions is that they don't down-weight
particles quite as fast. Additionally, the Laplace distribution has the
benefit of having a non-zero slope at the origin. This means that even
if the distribution is made overly broad, it will still distinguish between
particles that are near the origin. Results with real data compare 
the three distribution in \autoref{sec:Results Weights}.

After trial and error, I chose the weight standard deviation to be $.005$. Obviously
the case of the Cauchy, this is simply the scale parameter. While I 
did attempt to automatically set the standard deviation, I found that the algorithm
became too unpredictable. Essentially, if the weight function isn't
fixed across voxels, very noisy time series with no actual signal will 
converge to nonsensical results. 
In the future, it may be possible to set the standard deviation by
taking a small sample from "resting" data and using the sample standard deviation.
Since this is the first attempt at using particle filters for modeling the 
BOLD model, in this work I set the standard deviation manually at $.005$,
because it gives a more consistency and control. 

\subsection{Classical De-trending}
\label{sec:Detrend}
The non-stationary
aspect of a Weiner process, presumably the result of integrating some
$\nu_x$ is difficult to compensate for, and so various methods
have been developed to compensate for it. \cite{Tanabe2002} and \cite{Smith1999} have
demonstrated that this component is prevalent, and may in fact be an inherent  characteristic
of FMRI. In some studies, as many as half the voxels benefit from detrending, presenting
a serious barrier to inference (\cite{Smith2007}). All the existing methods are performed
during the preprocessing stage, rather than as an integral part of analyzing the BOLD
signal. There is no shortage of theories on the "best" method of detrending; however
in a head to head comparison, \cite{Tanabe2002}, showed that in most cases subtracting off
a spline works the best. The benefit of the spline versus wavelets, high pass 
filtering or other DC removal techniques is that the frequency response is not set.
Rather, the spline is adaptive to the input, having a low cut off if the signal's 
median stays constant but a high cut off frequency if the signal's median tends to
shift heavily over the course of time. In spite of this, the spline will still remove some
amount of signal, just like all of these methods, which is why I considered the method proposed
in \autoref{sec:Methods Delta Based Inference}.

The method I used to calculate the spline was picking one knot for every 15
measurements in an image. Thus a 10 minute session at a repetition time of 
2.1 seconds would have 19 knots. The knot first and last knots were each 
given half the number of samples as the rest of the knots; which were all 
located at the center of their sample group. The median of each sample group
was then taken and used as the magnitude for the group. Taking the median 
versus the mean seemed to work better, given the presence of outliers. It seems
that there is potential to optimize the spline further using a canonical 
HRF to find resting points; however, for this to work the experiment would have
to be designed with this in mind. 

When using the median-based spline techniques, the normalized signal will,
by definition have a median near zero. 
The problem with this is this is not the "natural" BOLD signal. More specifically,
when the signal is inactive, the BOLD response \emph{should} be at 0\% change from
the base level; activation may then increase, or for short periods decrease from this base.
After removing the spline, the BOLD resting state will be below 0\%.
This is problematic because it reduces the ability of an algorithm to learn.
One quick and dirty solution is to add an arbitrary constant to each BOLD response. 
Of course this won't scale to whole-brain analysis, so a more effective technique 
is adding a DC gain parameter to the BOLD model. 
Like all the other model parameters, given enough measurements, correct
value may be found. On the downside, adding another dimension increases the
complexity of the model, for a variable that is relatively obvious by direct
observation.

Thus, I used a more conventional approach to deal with this. To determine
the DC gain to be added to the signal I used a robust estimator 
of scale. The Median Absolute Deviation (MAD)
proved to be extremely accurate in determining how much to shift the signal up
by. I tested both methods during the course of analysis, and found that the increase 
in model complexity far outweighed the slight increase in flexibility. Its 
possible that a more accurate method may exist; however, for this case the 
MAD seems to work as it should, as \autoref{fig:DesplineQuality} shows. 

\begin{equation}
y_{\text{gain}, 0:K} = 2\text{median}_{i=0:K}(y_i - \text{median}(y_{0:K}))
\end{equation}

\begin{figure}
\caption{image of de-spline'd lines with "true" lines}
\label{fig:DesplineQuality}
\end{figure}

A serious concern with adding and subtracting arbitrary values to 
real data is whether this will create false positives. This is a legitimate
concern; however, all the additions and subtractions done in this section
have been very low frequency changes and should not substantially change
the BOLD response, being a relatively high frequency signal. The typical
method of preprocessing is a high-pass filter with a cutoff of around 30
seconds; which should about match the spline with knots every fifteen 
measurements. \cite{Tanabe2002} found that splines tended to far outperform the
high-pass filter method. 

\subsection{Delta Based Inference}
\label{sec:Methods Delta Based Inference}
The alternative to these sorts of low frequency manipulation is to
go around the problem in another way. Here, I propose and test a 
different method of dealing with the so called "drift". 
Instead of comparing the direct output of the particle filter with the direct
measurement, the algorithm would compare the change in signal over a single TR,
with the result of integrating the model for the same period. 
In most signal processing cases this would be foolish, but that is because the 
general assumption is that all noise is high frequency. Considering 
the fact that every BOLD analysis pipeline uses a high pass filter,
whereas low poss temporal filter are rarely applied, it makes sense
that a derivative type method could work. The benefit of particle filters
is that they are extremely robust method of inference, and I would assert 
that the particle filter ought to be given as \emph{raw} data as possible. 
While taking direct measurements
without de-trending would give awful results, using the difference removes the 
DC component and turns what is usually assumed to be a Weiner process into 
a simple Gaussian random variable. 

\begin{equation}
\Delta y = y(t) - y(t-1) = g(x(t)) - g(x(t-1)) + \nu_y(t) - \nu_y(t-1) + \nu_d(t) - \nu(t-1)
\label{eq:measass_delta}
\end{equation}

Even if $\nu_d$ is some other additive process, the difference will still be closer
to I.I.D. than a Wiener process, as the autocorrelation of the $\delta y$ shows
in \autoref{fig:QQDDelta} in \autoref{sec:Introduction Noise}. 
 All the assumptions made originally
for the particle filter still hold, and all of the parameters may be distinguished based on
the step sizes, thus it is not unreasonable to consider matching the string of step sizes
rather than string of direct readings. 

\begin{figure}
\label{fig:FrequencyGraphs}
\caption{frequency response graphs, highlighting noise frequency range and signal frequency range}
\end{figure}

\section{Preprocessing}
\label{sec:Methods Preprocessing}
As discussed in the section on de-trending, the normal pipeline for analyzing
FMRI involves a great deal of preprocessing. The first and most important
task is motion correction. To do this, a single volume in time is chosen, and
volumes at every other time are registered to this one volume. This takes care
of motion by the patient as well as some small changes in the magnetic
fields that cause the image to shift. After this, a structural image of the
patient is often registered to an atlas so that a grey-matter mask may
be created. Since all the "activation" should occur in grey matter voxels,
it made sense to restrict analysis to only these sections; although
there may be partially grey-matter areas that are missed. This significantly
eases computation time though. 

In normal statistical parametric mapping, a gaussian smoothing
filter is applied across the image as discussed in \autoref{sec:RFT}.
After this, detrending is performed discussed by \autoref{sec:Detrend}.
Recall that FMRI signal levels are unit-less and even though detrending isn't
always necessary, at the very least the data must be converted 
into \% difference from the baseline. Changing to 
\% difference removes no real information
from signal. This is the signal that was input into the delta based 
particle filter. Of course, most of the time analysis is performed on the
direct signal; which mandates the removal of low frequency drift.
The generally accepted method is to use a high pass filter, although the
cutoff frequency is application dependent and often applied haphazardly.

%For simulated and real images (tests with multiple time-series), tests were 
%also run with and without Gaussian filtering with sigma of %not sure todo
%were run, since it is standard
%practice to apply a Gaussian spatial filter to the images at each timestep. Obviously
%a spatial filter such as Gaussian filtering increased SNR but can also lead to less
%precision in the output maps.

\section{Simulation}
I performed two types of simulations. First, I simulated a single BOLD time-series based
on a randomly selected set of model parameters. This process was relatively straight forward,
given the state-space equations for the BOLD signal. After a "true" signal was generated,
I then added a carrier level, since BOLD is typically measured as a \% difference from the
base level. I tested the particle filter with two types of noise; independent and identically
distributed gaussian noise and weiner drift. For both I tested with various standard deviations.

Once this noisy simulated time series was generated, the exact same particle filter algorithm
that would later be run on full sized images, was run on this single voxel image. I ran
a series of tests to determine the convergence rate of the particle filter, the number
of particles that were required, how weighting functions compared, how different de-trending
methods compared with each other and, finally the variance of the result. By running the exact
time-series with different noise realizations, it was possible to determine the model variance.
As the reader may know, the error of an estimator may be calculated as:
\begin{equation}
MSE(\Theta) = Var(\Theta) + Bias(\Theta)^2
\end{equation}
The variance is an expression of how much the result would change for different noise realizations,
whereas the bias is an expression of how well the model matches the true underlying model. In
this case, because the same model is being used in the particle filter and underlying simulation,
the bias is actually zero. Obviously when this is calculated using \emph{real} data with an unknown
underlying state space equation, there will be some amount of bias error, but assuming that the
noise is similar to the noise used in these tests, the model variance will actually be about the
same. Thus calculating the model variance is extremely helpful in calculating how well determined
our model is, and how consistent it will be for real data. A single timeseries, as opposed to the
thousands present in a real image, makes it easier to 
compare the output with the ground truth, with various parameters. 

Second I used a modified version of the FSL tool 
POSSUM to generate an entire FMRI image from a parameter map. The parameter map was generated
by taking an existing activation map and assigning discrete parameter sets to each region.
The result was a four dimensional (length x width
x height x parameter) image with spatially varying parameters. Possum was then modified
to take a parameter map and generate activation levels depending on the parameters at that
point. The patch for POSSUM will be made available. As an unfortunate side effect of 
not using Possums' original activation scale, I manually added 750 to the total level of
simulated Possum images. This is because the BOLD \% difference levels were in the range
of 50 - 100\% from the base, about 5 times as large as they should have been. Ultimately
this should not have an effect on the parameters (other than perhaps $\epsilon$ and $V_0$). 
For each time-series in the simulated FMRI image, the final \emph{static} parameters were saved
into a parameter map. This parameter map may then be compared to the map used to generate the 
simulated data; additionally a new simulation using the calculated parameters may also be 
generated to test the difference in activation levels between the real parameters and the
estimated ones. Since it is clear that the parameters are not fully orthogonal 
(\cite{Deneux2006}), its possible that two sets of parameters are functionally equivalent,
but have different parameters. This way, an absolute 
quantitative difference between the two parameter sets may be found.

\section{Real Data}
Finally, we also performed inference based on real FMRI data. The scanner we used...
%... more specifics...

Before performing tests on a full image, I tested the results of the particle filter
on regions deemed active and non-active by statistical parametric mapping. 


