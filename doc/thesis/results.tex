\chapter{Results}

\section{Simulation}
I performed two types of simulations. First, I simulated a single BOLD time-series based
on a randomly selected set of model parameters. This process was relatively straight forward,
given the state-space equations for the BOLD signal. After a "true" signal was generated,
I then added a carrier level, since BOLD is typically measured as a \% difference from the
base level. I tested the particle filter with two types of noise; independent and identically
distributed gaussian noise and weiner drift. For both I tested with various standard deviations.

Once this noisy simulated time series was generated, the exact same particle filter algorithm
that would later be run on full sized images, was run on this single voxel image. I ran
a series of tests to determine the convergence rate of the particle filter, the number
of particles that were required, how weighting functions compared, how different de-trending
methods compared with each other and, finally the variance of the result. By running the exact
time-series with different noise realizations, it was possible to determine the model variance.
As the reader may know, the error of an estimator may be calculated as:
\begin{equation}
MSE(\Theta) = Var(\Theta) + Bias(\Theta)^2
\end{equation}
The variance is an expression of how much the result would change for different noise realizations,
whereas the bias is an expression of how well the model matches the true underlying model. In
this case, because the same model is being used in the particle filter and underlying simulation,
the bias is actually zero. Obviously when this is calculated using \emph{real} data with an unknown
underlying state space equation, there will be some amount of bias error, but assuming that the
noise is similar to the noise used in these tests, the model variance will actually be about the
same. Thus calculating the model variance is extremely helpful in calculating how well determined
our model is, and how consistent it will be for real data. A single timeseries, as opposed to the
thousands present in a real image, makes it easier to 
compare the output with the ground truth, with various parameters. 

Second I used a modified version of the FSL tool 
POSSUM to generate an entire FMRI image from a parameter map. The parameter map was generated
by taking an existing activation map and assigning discrete parameter sets to each region.
The result was a four dimensional (length x width
x height x parameter) image with spatially varying parameters. Possum was then modified
to take a parameter map and generate activation levels depending on the parameters at that
point. The patch for POSSUM will be made available. As an unfortunate side effect of 
not using Possums' original activation scale, I manually added 750 to the total level of
simulated Possum images. This is because the BOLD \% difference levels were in the range
of 50 - 100\% from the base, about 5 times as large as they should have been. Ultimately
this should not have an effect on the parameters (other than perhaps $\epsilon$ and $V_0$). 
For each time-series in the simulated FMRI image, the final \emph{static} parameters were saved
into a parameter map. This parameter map may then be compared to the map used to generate the 
simulated data; additionally a new simulation using the calculated parameters may also be 
generated to test the difference in activation levels between the real parameters and the
estimated ones. Since it is clear that the parameters are not fully orthogonal 
(\cite{Deneux2006}), its possible that two sets of parameters are functionally equivalent,
but have different parameters. This way, an absolute 
quantitative difference between the two parameter sets may be found.

\section{Real Data}
Finally, we also performed inference based on real FMRI data. The scanner we used...
%... more specifics...

Before performing tests on a full image, I tested the results of the particle filter
on regions deemed active and non-active by statistical parametric mapping. 
\section{Single-Voxel Simulation}
The results 

\section{Single-Voxel Analysis}
This section discusses the results when the particle filter was
applied on a single voxel. The parameters are the same as
those used later for entire image analysis; however, the results
are more in-depth. 

The run-time for a single voxel depends on the several factors. First, the
overall length of the signal being analyzed. For 1000 measurements it takes
about 10 to 15 minutes. On the other hand, in real circumstances the
length is only around 150 measurements. The number of  integration
can certainly make a large difference, however dropping below 1000 (.001 seconds)
is definitely not recommended. 

For a period I considered 1000 to be a
fine number; however when generating simulated data I found that every once
in a while 1000 was not enough. This is problematic in the actual particle
filter since, given the large number of simultaneous integrations taking 
place, its likely that a few particles will fail and be weighted at 0 because
of this problem. Additionally, because the typical case where a failure would
occur is at fast moving times/parameters the particles all tend to fail together.
The result is particle deprivation - no particles with non-zero weights remain.
The other possible outcome is that low time constant particles get pruned resulting
in excessively smoothed estimates for the time series'. Its possible that a
kind of stop-gap measure could be put into place; wherein particles that are
about to be set to NaN are integrated again with finer grained steps. However
many times the non-real results don't occur until several time steps after the 
numbers get strange. So for instance, the timestep was too long, allowing 
$f$ to go negative, resulting in extremely large values of $q$. There are many
different ways where this sort of event can occur, and unfortunately sometimes
there is no way to get back to before the state starting going out of control.

Another crucial factor to run time is how long before the first re-sampling 
occurs. Because the prior is represented initially with significantly more
particles, if the model fits very well, or for some reason the effective
number of particles stays high, resampling could take a long time to occur.
When this happens the particles filter can take a factor of 10 longer to run.
However, if the particle count isn't initially set high, there is a much larger
chance of particle deprivation occurring. Since there is no real way to know
how long it will take to resmaple the first time, there is little the 
algorithm can do to fix this. On the other end of the specture, if the time
series don't match the model at all, particle deprivation will occur extremely
quickly, and even the recovery technique discussed in \autoref{sec:Resampling}
won't help. The upshot of this is that the particle filter is able to 
identify these sections very quickly, and thus not waste much time there.
Of course, if a fat-tailed distribution is used for the weighting function,
or the standard deviation of a Gaussian weighting function is very large,
the particle filter will simply converge to meaningless values.

\begin{figure}
\caption{Particle Filter converging to values that make little sense,
because the voxel did not correlate with the input in any known way}
\end{figure}

\section{Weighting Function Comparison}
\label{sec:Results Weights}

\section{Single Time-Series Simulation}

Graphs: 

For simulated data, single timeseries:

For \{delta, DC/Spline\}, \{exponential, gaussian, cauchy\}, \{biased, unbiased initial\},
\{100, 500, 1000\} particles
\begin{enumerate}
\item Ground truth vs. Estimated signal during particle filter run
\item Ground truth vs. Estimated signal with final parameter set
\item True Parameters vs. Final Parameter Sets
\item Variance of final parameters when faced with same ground truth, different noise
\item MSE of (a new timeseries based on X(t) vs. ground truth) for all t
\item Estimator Variance based on different noise runs
\item Final Particle Distribution
\end{enumerate}

For Simulated Data, Full Volume:

%note to self, epsilon should probably be uniform between 0 and something
\section{Simulated Volume}
\begin{enumerate}
\item Parameter Map 
\item Error map of parameters
\item Histogram of \%errors between parameters
\item Activation Map based on a single region with high $\epsilon$, compared with linear
\end{enumerate}

Final parameter distribution among active regions.
Q-Q plots?

\section{FMRI Data}
....

image comparing epsilon-map with GLM activation map

